{"id": "2602.05072", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2602.05072", "abs": "https://arxiv.org/abs/2602.05072", "authors": ["Yuan-Pon Chen", "Olgica Milenkovic", "João Ribeiro", "Jin Sima"], "title": "Correcting Contextual Deletions in DNA Nanopore Readouts", "comment": "31 pages, 0 figures, 3 tables", "summary": "The problem of designing codes for deletion-correction and synchronization has received renewed interest due to applications in DNA-based data storage systems that use nanopore sequencers as readout platforms. In almost all instances, deletions are assumed to be imposed independently of each other and of the sequence context. These assumptions are not valid in practice, since nanopore errors tend to occur within specific contexts. We study contextual nanopore deletion-errors through the example setting of deterministic single deletions following (complete) runlengths of length at least $k$. The model critically depends on the runlength threshold $k$, and we examine two regimes for $k$: a) $k=C\\log n$ for a constant $C\\in(0,1)$; in this case, we study error-correcting codes that can protect from a constant number $t$ of contextual deletions, and show that the minimum redundancy (ignoring lower-order terms) is between $(1-C)t\\log n$ and $2(1-C)t\\log n$, meaning that it is a ($1-C$)-fraction of that of arbitrary $t$-deletion-correcting codes. To complement our non-constructive redundancy upper bound, we design efficiently and encodable and decodable codes for any constant $t$. In particular, for $t=1$ and $C>1/2$ we construct efficient codes with redundancy that essentially matches our non-constructive upper bound; b) $k$ equal a constant; in this case we consider the extremal problem where the number of deletions is not bounded and a deletion is imposed after every run of length at least $k$, which we call the extremal contextual deletion channel. This combinatorial setting arises naturally by considering a probabilistic channel that introduces contextual deletions after each run of length at least $k$ with probability $p$ and taking the limit $p\\to 1$. We obtain sharp bounds on the maximum achievable rate under the extremal contextual deletion channel for arbitrary constant $k$."}
{"id": "2602.05097", "categories": ["cs.IT", "math.AG"], "pdf": "https://arxiv.org/pdf/2602.05097", "abs": "https://arxiv.org/abs/2602.05097", "authors": ["Matteo Bonini", "Arianna Dionigi", "Francesco Ghiandoni"], "title": "On QC and GQC algebraic geometry codes", "comment": null, "summary": "We present new constructions of quasi-cyclic (QC) and generalized quasi-cyclic (GQC) codes from algebraic curves. Unlike previous approaches based on elliptic curves, our method applies to curves that are Kummer extensions of the rational function field, including hyperelliptic, norm-trace, and Hermitian curves. This allows QC codes with flexible co-index. Explicit parameter formulas are derived using known automorphism-group classifications."}
{"id": "2602.05405", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2602.05405", "abs": "https://arxiv.org/abs/2602.05405", "authors": ["Yi Chen", "Li Ming", "Chong Han"], "title": "Enabling Large-Scale Channel Sounding for 6G: A Framework for Sparse Sampling and Multipath Component Extraction", "comment": "13 pages", "summary": "Realizing the 6G vision of artificial intelligence (AI) and integrated sensing and communication (ISAC) critically requires large-scale real-world channel datasets for channel modeling and data-driven AI models. However, traditional frequency-domain channel sounding methods suffer from low efficiency due to a prohibitive number of frequency points to avoid delay ambiguity. This paper proposes a novel channel sounding framework involving sparse nonuniform sampling along with a likelihood-rectified space-alternating generalized expectation-maximization (LR-SAGE) algorithm for multipath component extraction. This framework enables the acquisition of channel datasets that are tens or even hundreds of times larger within the same channel measurement duration, thereby providing the massive data required to harness the full potential of AI scaling laws. Specifically, we propose a Parabolic Frequency Sampling (PFS) strategy that non-uniformly distributes frequency points, effectively eliminating delay ambiguity while reducing sampling overhead by orders of magnitude. To efficiently extract multipath components (MPCs) from the channel data measured by PFS, we develop a LR-SAGE algorithm, rectifying the likelihood distortion caused by nonuniform sampling and molecular absorption effect. Simulation results and experimental validation at 280--300~GHz confirm that the proposed PFS and LR-SAGE algorithm not only achieve 50$\\times$ faster measurement, a 98\\% reduction in data volume and a 99.96\\% reduction in post-processing computational complexity, but also successfully captures MPCs and channel characteristics consistent with traditional exhaustive measurements, demonstrating its potential as a fundamental enabler for constructing the massive ISAC datasets required by AI-native 6G systems."}
{"id": "2602.05462", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2602.05462", "abs": "https://arxiv.org/abs/2602.05462", "authors": ["Kuo Shang", "Chen Yuan", "Ruiqi Zhu"], "title": "Explicit List-Decodable Linearized Reed-Solomon Subspace Codes via Subspace Designs", "comment": "28 pages", "summary": "The sum-rank metric is the mixture of the Hamming and rank metrics. The sum-rank metric found its application in network coding, locally repairable codes, space-time coding, and quantum-resistant cryptography. Linearized Reed-Solomon (LRS) codes are the sum-rank analogue of Reed-Solomon codes and strictly generalize both Reed-Solomon and Gabidulin codes.\n  In this work, we construct an explicit family of $\\mathbb{F}_h$-linear sum-rank metric codes over arbitrary fields $\\mathbb{F}_h$. Our construction enables efficient list decoding up to a fraction $ρ$ of errors in the sum-rank metric with rate $1-ρ-\\varepsilon$, for any desired $ρ\\in (0,1)$ and $\\varepsilon>0$. Our codes are subcodes of LRS codes, obtained by restricting message polynomials to an $\\mathbb{F}_h$-subspace derived from subspace designs, and the decoding list size is bounded by $h^{\\mathrm{poly}(1/\\varepsilon)}$.\n  Beyond the standard LRS setting, we further extend our linear-algebraic decoding framework to folded Linearized Reed-Solomon (FLRS) codes. We show that folded evaluations satisfy appropriate interpolation conditions and that the corresponding solution space forms a low-dimensional, structured affine subspace. This structure enables effective control of the list size and yields the first explicit positive-rate FLRS subcodes that are efficiently list decodable beyond the unique-decoding radius. To the best of our knowledge, this also constitutes the first explicit construction of positive-rate sum-rank metric codes that admit efficient list decoding beyond the unique decoding radius, thereby providing a new general framework for constructing efficiently decodable codes under the sum-rank metric."}
{"id": "2602.05034", "categories": ["eess.SP", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.05034", "abs": "https://arxiv.org/abs/2602.05034", "authors": ["Fatih Ayten", "Musa Furkan Keskin", "Akshay Jain", "Mehmet C. Ilter", "Ossi Kaltiokallio", "Jukka Talvitie", "Elena Simona Lohan", "Mikko Valkama"], "title": "Phase-Only Positioning in Distributed MIMO Under Phase Impairments: AP Selection Using Deep Learning", "comment": null, "summary": "Carrier phase positioning (CPP) can enable cm-level accuracy in next-generation wireless systems, while recent literature shows that accuracy remains high using phase-only measurements in distributed MIMO (D-MIMO). However, the impact of phase synchronization errors on such systems remains insufficiently explored. To address this gap, we first show that the proposed hyperbola intersection method achieves highly accurate positioning even in the presence of phase synchronization errors, when trained on appropriate data reflecting such impairments. We then introduce a deep learning (DL)-based D-MIMO antenna point (AP) selection framework that ensures high-precision localization under phase synchronization errors. Simulation results show that the proposed framework improves positioning accuracy compared to prior-art methods, while reducing inference complexity by approximately 19.7%."}
{"id": "2602.05666", "categories": ["cs.IT", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.05666", "abs": "https://arxiv.org/abs/2602.05666", "authors": ["Chao Zhou", "Changsheng You", "Cong Zhou", "Li Chen", "Yi Gong", "Chengwen Xing"], "title": "Low-complexity Design for Beam Coverage in Near-field and Far-field: A Fourier Transform Approach", "comment": "13 pages, 7 figures, submitted to IEEE for possible publication", "summary": "In this paper, we study efficient beam coverage design for multi-antenna systems in both far-field and near-field cases. To reduce the computational complexity of existing sampling-based optimization methods, we propose a new low-complexity yet efficient beam coverage design. To this end, we first formulate a general beam coverage optimization problem to maximize the worst-case beamforming gain over a target region. For the far-field case, we show that the beam coverage design can be viewed as a spatial-frequency filtering problem, where angular coverage can be achieved by weight-shaping in the antenna domain via an inverse FT, yielding an infinite-length weighting sequence. Under the constraint of a finite number of antennas, a surrogate scheme is proposed by directly truncating this sequence, which inevitably introduces a roll-off effect at the angular boundaries, yielding degraded worst-case beamforming gain. To address this issue, we characterize the finite-antenna-induced roll-off effect, based on which a roll-off-aware design with a protective zoom is developed to ensure a flat beamforming-gain profile within the target angular region. Next, we extend the proposed method to the near-field case. Specifically, by applying a first-order Taylor approximation to the near-field channel steering vector (CSV), the two-dimensional (2D) beam coverage design (in both angle and inverse-range) can be transformed into a 2D inverse FT, leading to a low-complexity beamforming design. Furthermore, an inherent near-field range defocusing effect is observed, indicating that sufficiently wide angular coverage results in range-insensitive beam steering. Finally, numerical results demonstrate that the proposed FT-based approach achieves a comparable worst-case beamforming performance with that of conventional sampling-based optimization methods while significantly reducing the computational complexity."}
{"id": "2602.05209", "categories": ["eess.SP", "cs.IT"], "pdf": "https://arxiv.org/pdf/2602.05209", "abs": "https://arxiv.org/abs/2602.05209", "authors": ["Zhiyu Chen", "Ming-Min Zhao", "Songfu Cai", "Ming Lei", "Min-Jian Zhao"], "title": "Integrated Sensing, Communication, and Control for UAV-Assisted Mobile Target Tracking", "comment": "13 pages, 10 figures", "summary": "Unmanned aerial vehicles (UAVs) are increasingly deployed in mission-critical applications such as target tracking, where they must simultaneously sense dynamic environments, ensure reliable communication, and achieve precise control. A key challenge here is to jointly guarantee tracking accuracy, communication reliability, and control stability within a unified framework. To address this issue, we propose an integrated sensing, communication, and control (ISCC) framework for UAV-assisted target tracking, where the considered tracking system is modeled as a discrete-time linear control process, with the objective of driving the deviation between the UAV and target states toward zero. We formulate a stochastic model predictive control (MPC) optimization problem for joint control and beamforming design, which is highly non-convex and intractable in its original form. To overcome this difficulty, the target state is first estimated using an extended Kalman filter (EKF). Then, by deriving the closed-form optimal beamforming solution under a given control input, the original problem is equivalently reformulated into a tractable control-oriented form. Finally, we convexify the remaining non-convex constraints via a relaxation-based convex approximation, yielding a computationally tractable convex optimization problem that admits efficient global solution. Numerical results show that the proposed ISCC framework achieves tracking accuracy comparable to a non-causal benchmark while maintaining stable communication, and it significantly outperforms the conventional control and tracking method."}
{"id": "2602.05744", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2602.05744", "abs": "https://arxiv.org/abs/2602.05744", "authors": ["Guglielmo Beretta", "Tommaso Cesari", "Roberto Colomboni"], "title": "Generalized Pinsker Inequality for Bregman Divergences of Negative Tsallis Entropies", "comment": null, "summary": "The Pinsker inequality lower bounds the Kullback--Leibler divergence $D_{\\textrm{KL}}$ in terms of total variation and provides a canonical way to convert $D_{\\textrm{KL}}$ control into $\\lVert \\cdot \\rVert_1$-control. Motivated by applications to probabilistic prediction with Tsallis losses and online learning, we establish a generalized Pinsker inequality for the Bregman divergences $D_α$ generated by the negative $α$-Tsallis entropies -- also known as $β$-divergences. Specifically, for any $p$, $q$ in the relative interior of the probability simplex $Δ^K$, we prove the sharp bound \\[\n  D_α(p\\Vert q) \\ge \\frac{C_{α,K}}{2}\\cdot \\|p-q\\|_1^2, \\] and we determine the optimal constant $C_{α,K}$ explicitly for every choice of $(α,K)$."}
{"id": "2602.05308", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.05308", "abs": "https://arxiv.org/abs/2602.05308", "authors": ["Jiwei Qian", "Yee Hui Lee", "Kaixuan Cheng", "Qiqi Dai", "Arda Yalcinkaya", "Mohamed Lokman Mohd Yusof", "James Wang", "Abdulkadir C. Yucel"], "title": "A Migration-Assisted Deep Learning Scheme for Imaging Defects Inside Cylindrical Structures via GPR: A Case Study for Tree Trunks", "comment": null, "summary": "Ground-penetrating radar (GPR) has emerged as a prominent tool for imaging internal defects in cylindrical structures, such as columns, utility poles, and tree trunks. However, accurately reconstructing both the shape and permittivity of the defects inside cylindrical structures remains challenging due to complex wave scattering phenomena and the limited accuracy of the existing signal processing and deep learning techniques. To address these issues, this study proposes a migration-assisted deep learning scheme for reconstructing the shape and permittivity of defects within cylindrical structures. The proposed scheme involves three stages of GPR data processing. First, a dual-permittivity estimation network extracts the permittivity values of the defect and the cylindrical structure, the latter of which is estimated with the help of a novel structural similarity index measure-based autofocusing technique. Second, a modified Kirchhoff migration incorporating the extracted permittivity of the cylindrical structure maps the signals reflected from the defect to the imaging domain. Third, a shape reconstruction network processes the migrated image to recover the precise shape of the defect. The image of the interior defect is finally obtained by combining the reconstructed shape and extracted permittivity of the defect. The proposed scheme is validated using both synthetic and experimental data from a laboratory trunk model and real tree trunk samples. Comparative results show superior performance over existing deep learning methods, while generalization tests on live trees confirm its feasibility for in-field deployment. The underlying principle can further be applied to other circumferential GPR imaging scenarios. The code and database are available at: https://github.com/jwqian54/Migration-Assisted-DL."}
{"id": "2602.05751", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2602.05751", "abs": "https://arxiv.org/abs/2602.05751", "authors": ["Ravi Sharan Bhagavathula", "Pavan Koteshwar Srinath", "Alvaro Valcarce Rial", "Baltasar-Beferull Lozano"], "title": "MU-MIMO Uplink Timely Throughput Maximization for Extended Reality Applications", "comment": "14 pages, single column, 4 figures. This work has been submitted to the IEEE for possible publication", "summary": "In this work, we study the cross-layer timely throughput maximization for extended reality (XR) applications through uplink multi-user MIMO (MU-MIMO) scheduling. Timely scheduling opportunities are characterized by the peak age of information (PAoI)-metric and are incorporated into a network-side optimization problem as constraints modeling user satisfaction. The problem being NP-hard, we resort to a signaling-free, weighted proportional fair-based iterative heuristic algorithm, where the weights are derived with respect to the PAoI metric. Extensive numerical simulation results demonstrate that the proposed algorithm consistently outperforms existing baselines in terms of XR capacity without sacrificing the overall system throughput."}
{"id": "2602.05342", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.05342", "abs": "https://arxiv.org/abs/2602.05342", "authors": ["Zitong Wang", "Cheng Zhang", "Wen Wang", "Shuigen Yang", "Haiming Wang", "Yongming Huang"], "title": "Joint Optimization of Latency and Accuracy for Split Federated Learning in User-Centric Cell-Free MIMO Networks", "comment": null, "summary": "This paper proposes a user-centric split federated learning (UCSFL) framework for user-centric cell-free multiple-input multiple-output (CF-MIMO) networks to support split federated learning (SFL). In the proposed UCSFL framework, users deploy split sub-models locally, while complete models are maintained and updated at access point (AP)-side distributed processing units (DPUs), followed by a two-level aggregation procedure across DPUs and the central processing unit (CPU). Under standard machine learning (ML) assumptions, we provide a theoretical convergence analysis for UCSFL, which reveals that the AP-cluster size is a key factor influencing model training accuracy. Motivated by this result, we introduce a new performance metric, termed the latency-to-accuracy ratio, defined as the ratio of a user's per-iteration training latency to the weighted size of its AP cluster. Based on this metric, we formulate a joint optimization problem to minimize the maximum latency-to-accuracy ratio by jointly optimizing uplink power control, downlink beamforming, model splitting, and AP clustering. The resulting problem is decomposed into two sub-problems operating on different time scales, for which dedicated algorithms are developed to handle the short-term and long-term optimizations, respectively. Simulation results verify the convergence of the proposed algorithms and demonstrate that UCSFL effectively reduces the latency-to-accuracy ratio of the VGG16 model compared with baseline schemes. Moreover, the proposed framework adaptively adjusts splitting and clustering strategies in response to varying communication and computation resources. An MNIST-based handwritten digit classification example further shows that UCSFL significantly accelerates the convergence of the VGG16 model."}
{"id": "2602.05790", "categories": ["cs.IT", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.05790", "abs": "https://arxiv.org/abs/2602.05790", "authors": ["Alina Harbuzova", "Or Ordentlich", "Yury Polyanskiy"], "title": "Price of universality in vector quantization is at most 0.11 bit", "comment": "41 page, 1 figure", "summary": "Fast computation of a matrix product $W^\\top X$ is a workhorse of modern LLMs. To make their deployment more efficient, a popular approach is that of using a low-precision approximation $\\widehat W$ in place of true $W$ (\"weight-only quantization''). Information theory demonstrates that an optimal algorithm for reducing precision of $W$ depends on the (second order) statistics of $X$ and requires a careful alignment of vector quantization codebook with PCA directions of $X$ (a process known as \"waterfilling allocation''). Dependence of the codebook on statistics of $X$, however, is highly impractical. This paper proves that there exist a universal codebook that is simultaneously near-optimal for all possible statistics of $X$, in the sense of being at least as good as an $X$-adapted waterfilling codebook with rate reduced by 0.11 bit per dimension. Such universal codebook would be an ideal candidate for the low-precision storage format, a topic of active modern research, but alas the existence proof is non-constructive.\n  Equivalently, our result shows existence of a net in $\\mathbb{R}^n$ that is a nearly-optimal covering of a sphere simultaneously with respect to all Hilbert norms."}
{"id": "2602.05554", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.05554", "abs": "https://arxiv.org/abs/2602.05554", "authors": ["Mohammad Shamsesalehi", "Mahmoud Ahmadian Attari", "Mohammad Amin Maleki Sadr", "Benoit Champagne"], "title": "Beamformed Fingerprint-Based Transformer Network for Trajectory Estimation and Path Determination in Outdoor mmWave MIMO Systems", "comment": "14 pages, 11 figures", "summary": "Radio transmissions in millimeter wave (mmWave) bands have gained significant interest for applications demanding precise device localization and trajectory estimation. This paper explores novel neural network (NN) architectures suitable for trajectory estimation and path determination in a mmWave multiple-input multiple-output (MIMO) outdoor system based on localization data from beamformed fingerprint (BFF). The NN architecture captures sequences of BFF signals from different users, and through the application of learning mechanisms, subsequently estimate their trajectories. In turn, this information is employed to find the shortest path to the target, thereby enabling more efficient navigation. Specifically, we propose a two-stage procedure for trajectory estimation and optimal path finding. In the first stage, a transformer network (TN) based on attention mechanisms is developed to predict trajectories of wireless devices using BFF sequences captured in a mmWave MIMO outdoor system. In the second stage, a novel algorithm based on Informed Rapidly-exploring Random Trees (iRRT*) is employed to determine the optimal path to target locations using trajectory estimates derived in the first stage. The effectiveness of the proposed schemes is validated through numerical experiments, using a comprehensive dataset of radio measurements, generated using ray tracing simulations to model outdoor propagation at 28 GHz. We show that our proposed TN-based trajectory estimator outperforms other methods from the recent literature and can successfully generalize to new trajectories outside the training set. Furthermore, our proposed iRRT* algorithm is able to consistently provide the shortest path to the target."}
{"id": "2602.05209", "categories": ["eess.SP", "cs.IT"], "pdf": "https://arxiv.org/pdf/2602.05209", "abs": "https://arxiv.org/abs/2602.05209", "authors": ["Zhiyu Chen", "Ming-Min Zhao", "Songfu Cai", "Ming Lei", "Min-Jian Zhao"], "title": "Integrated Sensing, Communication, and Control for UAV-Assisted Mobile Target Tracking", "comment": "13 pages, 10 figures", "summary": "Unmanned aerial vehicles (UAVs) are increasingly deployed in mission-critical applications such as target tracking, where they must simultaneously sense dynamic environments, ensure reliable communication, and achieve precise control. A key challenge here is to jointly guarantee tracking accuracy, communication reliability, and control stability within a unified framework. To address this issue, we propose an integrated sensing, communication, and control (ISCC) framework for UAV-assisted target tracking, where the considered tracking system is modeled as a discrete-time linear control process, with the objective of driving the deviation between the UAV and target states toward zero. We formulate a stochastic model predictive control (MPC) optimization problem for joint control and beamforming design, which is highly non-convex and intractable in its original form. To overcome this difficulty, the target state is first estimated using an extended Kalman filter (EKF). Then, by deriving the closed-form optimal beamforming solution under a given control input, the original problem is equivalently reformulated into a tractable control-oriented form. Finally, we convexify the remaining non-convex constraints via a relaxation-based convex approximation, yielding a computationally tractable convex optimization problem that admits efficient global solution. Numerical results show that the proposed ISCC framework achieves tracking accuracy comparable to a non-causal benchmark while maintaining stable communication, and it significantly outperforms the conventional control and tracking method."}
{"id": "2602.05560", "categories": ["eess.SP", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2602.05560", "abs": "https://arxiv.org/abs/2602.05560", "authors": ["Yangjin Xu", "Wei Gao", "Xiaolei Li", "Qinghang Zeng"], "title": "Depth estimation of a monoharmonic source using a vertical linear array at fixed distance", "comment": null, "summary": "Estimating the depth of a monoharmonic sound source at a fixed range using a vertical linear array (VLA) is challenging in the absence of seabed environmental parameters, and relevant research remains scarce. The orthogonality constrained modal search based depth estimation (OCMS-D) method is proposed in this paper, which enables the estimation of the depth of a monoharmonic source at a fixed range using a VLA under unknown seabed parameters. Using the sparsity of propagating normal modes and the orthogonality of mode depth functions, OCMS-D estimates the normal mode parameters under a fixed source-array distance at first. The estimated normal mode parameters are then used to estimate the source depth. To ensure the precision of the source depth estimation, the method utilizes information on both the amplitude distribution and the sign (positive/negative) patterns of the estimated mode depth functions at the inferred source depth. Numerical simulations evaluate the performance of OCMS-D under different conditions. The effectiveness of OCMS-D is also verified by the Yellow Sea experiment and the SWellEx-96 experiment. In the Yellow Sea experiment, the depth estimation absolute errors by OCMS-D with a 4-second time window are less than 2.4 m. And the depth estimation absolute errors in the SWellEx-96 experiment with a 10-second time window are less than 5.4 m for the shallow source and less than 10.8 m for the deep source."}
{"id": "2602.05724", "categories": ["eess.SP", "cs.IT"], "pdf": "https://arxiv.org/pdf/2602.05724", "abs": "https://arxiv.org/abs/2602.05724", "authors": ["Shoma Hara", "Takumi Takahashi", "Hiroki Iimori", "Hideki Ochiai", "Erik G. Larsson"], "title": "Reciprocity Calibration of Dual-Antenna Repeaters via MMSE Estimation", "comment": "13 pages, 9 figures", "summary": "This paper proposes a novel Bayesian reciprocity calibration method that consistently ensures uplink and downlink channel reciprocity in repeater-assisted multiple-input multiple-output (MIMO) systems. The proposed algorithm is formulated under the minimum mean-square error (MMSE) criterion. Its Bayesian framework incorporates complete statistical knowledge of the signal model, noise, and prior distributions, enabling a coherent design that achieves both low computational complexity and high calibration accuracy. To further enhance phase alignment accuracy, which is critical for calibration tasks, we develop a von Mises denoiser that exploits the fact that the target parameters lie on the circle in the complex plane. Simulation results demonstrate that the proposed MMSE algorithm achieves substantially improved estimation accuracy compared with conventional deterministic non-linear least-squares (NLS) methods, while maintaining comparable computational complexity. Furthermore, the proposed method exhibits remarkably fast convergence, making it well suited for practical implementation."}
{"id": "2602.05579", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.05579", "abs": "https://arxiv.org/abs/2602.05579", "authors": ["Mu Jia", "Hao Sun", "Junting Chen", "Pooi-Yuen Kam"], "title": "Physics-Aware Tensor Reconstruction for Radio Maps in Pixel-Based Fluid Antenna Systems", "comment": null, "summary": "The deployment of pixel-based antennas and fluid antenna systems (FAS) is hindered by prohibitive channel state information (CSI) acquisition overhead. While radio maps enable proactive mode selection, reconstructing high-fidelity maps from sparse measurements is challenging. Existing physics-agnostic or data-driven methods often fail to recover fine-grained shadowing details under extreme sparsity. We propose a Physics-Regularized Low-Rank Tensor Completion (PR-LRTC) framework for radio map reconstruction. By modeling the signal field as a three-way tensor, we integrate environmental low-rankness with deterministic antenna physics. Specifically, we leverage Effective Aerial Degrees-of-Freedom (EADoF) theory to derive a differential gain topology map as a physical prior for regularization. The resulting optimization problem is solved via an efficient Alternating Direction Method of Multipliers (ADMM)-based algorithm. Simulations show that PR-LRTC achieves a 4 dB gain over baselines at a 10% sampling ratio. It effectively preserves sharp shadowing edges, providing a robust, physics-compliant solution for low-overhead beam management."}
{"id": "2602.05802", "categories": ["eess.SP", "cs.IT"], "pdf": "https://arxiv.org/pdf/2602.05802", "abs": "https://arxiv.org/abs/2602.05802", "authors": ["Niclas Führling", "Getuar Rexhepi", "Giuseppe Abreu"], "title": "Discrete Aware Tensor Completion via Convexized $\\ell_0$-Norm Approximation", "comment": null, "summary": "We consider a novel algorithm, for the completion of partially observed low-rank tensors, where each entry of the tensor can be chosen from a discrete finite alphabet set, such as in common image processing problems, where the entries represent the RGB values. The proposed low-rank tensor completion (TC) method builds on the conventional nuclear norm (NN) minimization-based low-rank TC paradigm, through the addition of a discrete-aware regularizer, which enforces discreteness in the objective of the problem, by an $\\ell_0$-norm regularizer that is approximated by a continuous and differentiable function normalized via fractional programming (FP) under a proximal gradient (PG) framework, in order to solve the proposed problem. Simulation results demonstrate the superior performance of the new method both in terms of normalized mean square error (NMSE) and convergence, compared to the conventional state of-the-art (SotA) techniques, including NN minimization approaches, as well as a mixture of the latter with a matrix factorization approach."}
{"id": "2602.05581", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.05581", "abs": "https://arxiv.org/abs/2602.05581", "authors": ["Ziqing Xing", "Zhaoyang Zhang", "Xin Tong", "Zhaohui Yang", "Chongwen Huang"], "title": "Physics-Inspired Target Shape Detection and Reconstruction in mmWave Communication Systems", "comment": "Accepted by GLOBECOM 2023", "summary": "The integration of sensing and communication (ISAC) is an essential function of future wireless systems. Due to its large available bandwidth, millimeter-wave (mmWave) ISAC systems are able to achieve high sensing accuracy. In this paper, we consider the multiple base-station (BS) collaborative sensing problem in a multi-input multi-output (MIMO) orthogonal frequency division multiplexing (OFDM) mmWave communication system. Our aim is to sense a remote target shape with the collected signals which consist of both the reflection and scattering signals. We first characterize the mmWave's scattering and reflection effects based on the Lambertian scattering model. Then we apply the periodogram technique to obtain rough scattering point detection, and further incorporate the subspace method to achieve more precise scattering and reflection point detection. Based on these, a reconstruction algorithm based on Hough Transform and principal component analysis (PCA) is designed for a single convex polygon target scenario. To improve the accuracy and completeness of the reconstruction results, we propose a method to further fuse the scattering and reflection points. Extensive simulation results validate the effectiveness of the proposed algorithms."}
{"id": "2602.05715", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.05715", "abs": "https://arxiv.org/abs/2602.05715", "authors": ["Yuyang Liu", "Johan Karlsson", "Filip Elvander"], "title": "Sound Field Estimation Using Optimal Transport Barycenters in the Presence of Phase Errors", "comment": null, "summary": "This study introduces a novel approach for estimating plane-wave coefficients in sound field reconstruction, specifically addressing challenges posed by error-in-variable phase perturbations. Such systematic errors typically arise from sensor mis-calibration, including uncertainties in sensor positions and response characteristics, leading to measurement-induced phase shifts in plane wave coefficients. Traditional methods often result in biased estimates or non-convex solutions. To overcome these issues, we propose an optimal transport (OT) framework. This framework operates on a set of lifted non-negative measures that correspond to observation-dependent shifted coefficients relative to the unperturbed ones. By applying OT, the supports of the measures are transported toward an optimal average in the phase space, effectively morphing them into an indistinguishable state. This optimal average, known as barycenter, is linked to the estimated plane-wave coefficients using the same lifting rule. The framework addresses the ill-posed nature of the problem, due to the large number of plane waves, by adding a constant to the ground cost, ensuring the sparsity of the transport matrix. Convex consistency of the solution is maintained. Simulation results confirm that our proposed method provides more accurate coefficient estimations compared to baseline approaches in scenarios with both additive noise and phase perturbations."}
{"id": "2602.05724", "categories": ["eess.SP", "cs.IT"], "pdf": "https://arxiv.org/pdf/2602.05724", "abs": "https://arxiv.org/abs/2602.05724", "authors": ["Shoma Hara", "Takumi Takahashi", "Hiroki Iimori", "Hideki Ochiai", "Erik G. Larsson"], "title": "Reciprocity Calibration of Dual-Antenna Repeaters via MMSE Estimation", "comment": "13 pages, 9 figures", "summary": "This paper proposes a novel Bayesian reciprocity calibration method that consistently ensures uplink and downlink channel reciprocity in repeater-assisted multiple-input multiple-output (MIMO) systems. The proposed algorithm is formulated under the minimum mean-square error (MMSE) criterion. Its Bayesian framework incorporates complete statistical knowledge of the signal model, noise, and prior distributions, enabling a coherent design that achieves both low computational complexity and high calibration accuracy. To further enhance phase alignment accuracy, which is critical for calibration tasks, we develop a von Mises denoiser that exploits the fact that the target parameters lie on the circle in the complex plane. Simulation results demonstrate that the proposed MMSE algorithm achieves substantially improved estimation accuracy compared with conventional deterministic non-linear least-squares (NLS) methods, while maintaining comparable computational complexity. Furthermore, the proposed method exhibits remarkably fast convergence, making it well suited for practical implementation."}
{"id": "2602.05802", "categories": ["eess.SP", "cs.IT"], "pdf": "https://arxiv.org/pdf/2602.05802", "abs": "https://arxiv.org/abs/2602.05802", "authors": ["Niclas Führling", "Getuar Rexhepi", "Giuseppe Abreu"], "title": "Discrete Aware Tensor Completion via Convexized $\\ell_0$-Norm Approximation", "comment": null, "summary": "We consider a novel algorithm, for the completion of partially observed low-rank tensors, where each entry of the tensor can be chosen from a discrete finite alphabet set, such as in common image processing problems, where the entries represent the RGB values. The proposed low-rank tensor completion (TC) method builds on the conventional nuclear norm (NN) minimization-based low-rank TC paradigm, through the addition of a discrete-aware regularizer, which enforces discreteness in the objective of the problem, by an $\\ell_0$-norm regularizer that is approximated by a continuous and differentiable function normalized via fractional programming (FP) under a proximal gradient (PG) framework, in order to solve the proposed problem. Simulation results demonstrate the superior performance of the new method both in terms of normalized mean square error (NMSE) and convergence, compared to the conventional state of-the-art (SotA) techniques, including NN minimization approaches, as well as a mixture of the latter with a matrix factorization approach."}
{"id": "2602.05876", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.05876", "abs": "https://arxiv.org/abs/2602.05876", "authors": ["Chenyang Yan", "Mats Bengtsson"], "title": "IDSOR: Intensity- and Distance-Aware Statistical Outlier Removal for Weather-Robust LiDAR Point Clouds", "comment": null, "summary": "LiDAR point clouds captured in rain or snow are often corrupted by weather-induced returns, which can degrade perception and safety-critical scene understanding. This paper proposes Intensity- and Distance-Aware Statistical Outlier Removal (IDSOR), a range-adaptive filtering method that jointly exploits intensity cues and neighborhood sparsity. By incorporating an empirical, range-dependent distribution of weather returns into the threshold design, IDSOR suppresses weather-induced points while preserving fine structural details without cumbersome manual parameter tuning. We also propose a variant that uses a previously proposed method to estimate the weather return distribution from data, and integrates it into IDSOR. Experiments on simulation-augmented level-crossing measurements and on the Winter Adverse Driving dataset (WADS) demonstrate that IDSOR achieves a favorable precision-recall trade-off, maintaining both precision and recall above 90% on WADS."}
{"id": "2602.05666", "categories": ["cs.IT", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.05666", "abs": "https://arxiv.org/abs/2602.05666", "authors": ["Chao Zhou", "Changsheng You", "Cong Zhou", "Li Chen", "Yi Gong", "Chengwen Xing"], "title": "Low-complexity Design for Beam Coverage in Near-field and Far-field: A Fourier Transform Approach", "comment": "13 pages, 7 figures, submitted to IEEE for possible publication", "summary": "In this paper, we study efficient beam coverage design for multi-antenna systems in both far-field and near-field cases. To reduce the computational complexity of existing sampling-based optimization methods, we propose a new low-complexity yet efficient beam coverage design. To this end, we first formulate a general beam coverage optimization problem to maximize the worst-case beamforming gain over a target region. For the far-field case, we show that the beam coverage design can be viewed as a spatial-frequency filtering problem, where angular coverage can be achieved by weight-shaping in the antenna domain via an inverse FT, yielding an infinite-length weighting sequence. Under the constraint of a finite number of antennas, a surrogate scheme is proposed by directly truncating this sequence, which inevitably introduces a roll-off effect at the angular boundaries, yielding degraded worst-case beamforming gain. To address this issue, we characterize the finite-antenna-induced roll-off effect, based on which a roll-off-aware design with a protective zoom is developed to ensure a flat beamforming-gain profile within the target angular region. Next, we extend the proposed method to the near-field case. Specifically, by applying a first-order Taylor approximation to the near-field channel steering vector (CSV), the two-dimensional (2D) beam coverage design (in both angle and inverse-range) can be transformed into a 2D inverse FT, leading to a low-complexity beamforming design. Furthermore, an inherent near-field range defocusing effect is observed, indicating that sufficiently wide angular coverage results in range-insensitive beam steering. Finally, numerical results demonstrate that the proposed FT-based approach achieves a comparable worst-case beamforming performance with that of conventional sampling-based optimization methods while significantly reducing the computational complexity."}
