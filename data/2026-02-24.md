<div id=toc></div>

# Table of Contents

- [cs.IT](#cs.IT) [Total: 8]
- [eess.SP](#eess.SP) [Total: 20]


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [1] [Turbo Coded Single Sideband OFDM-OQAM Signaling through Frequency Selective Rayleigh Fading Channels](https://arxiv.org/abs/2602.18881)
*Kasturi Vasudevan*

Main category: cs.IT

TL;DR: 研究Turbo编码OFDM-OQAM系统在频率选择性瑞利衰落信道中的BER性能，提出使用RRC脉冲及其希尔伯特变换作为复值发射滤波器，采用类似SSB调制的结构，并开发了帧检测、CFO估计、信道估计等算法。


<details>
  <summary>Details</summary>
Motivation: 研究在存在载波频率偏移(CFO)和加性高斯白噪声(AWGN)的频率选择性瑞利衰落信道中，Turbo编码OFDM-OQAM信号的BER性能，填补该领域的研究空白。

Method: 使用根升余弦(RRC)脉冲及其希尔伯特变换作为复值发射滤波器，接收端采用简单匹配滤波器，系统结构类似单边带(SSB)调制。采用Turbo编码和子载波分集技术，提出了帧检测、两步CFO估计、信道估计和噪声方差估计的离散时间算法。

Result: 提出的系统在频率选择性瑞利衰落信道中表现出改善的BER性能，相比未编码系统有显著提升。提出的估计算法能够有效处理CFO和信道估计问题。

Conclusion: 该工作首次研究了Turbo编码OFDM-OQAM系统在CFO和AWGN存在下的性能，提出的RRC脉冲滤波器和SSB类似结构有效改善了系统性能，填补了该领域的研究空白。

Abstract: This work investigates the bit-error-rate (BER) performance of turbo coded orthogonal frequency division multiplexed - offset quadrature amplitude modulated (OFDM- OQAM) signals transmitted through frequency selective Rayleigh fading channels in the presence of carrier frequency offset (CFO) and additive white Gaussian noise (AWGN). The highlight of this work is to use the root raised cosine (RRC) pulse and its Hilbert transform as the complex-valued transmit filter and a simple matched filter at the receiver. The proposed system is similar to single sideband (SSB) modulation, that has roots in analog communications. Turbo code and subcarrier diversity is employed to improve the BER performance over that of an uncoded system. Discrete-time algorithms for frame detection, two-step CFO, channel and noise variance estimation have been proposed. A single transmit and receive antenna is assumed. Similar work has not been done earlier.

</details>


### [2] [Derivation Depth as an Information Metric: Axioms, Coding Theorems, and Storage--Computation Tradeoffs](https://arxiv.org/abs/2602.19137)
*Jianfeng Xu*

Main category: cs.IT

TL;DR: 本文引入推导深度作为衡量基于给定前提回答查询所需推理努力的可计算度量，建立了描述复杂性与推导深度的基本界限，并提出了实用的存储-计算权衡策略。


<details>
  <summary>Details</summary>
Motivation: 需要一种可计算的度量来量化回答查询所需的推理努力，并理解知识表示、推理复杂性和计算效率之间的基本关系。

Method: 将信息建模为连接抽象知识与物理载体的双层结构，分离核心事实与操作捷径。定义推导深度并证明其可计算性，通过编码推理轨迹和应用信息论不可压缩性论证建立基本界限。

Result: 建立了推导深度与查询描述复杂性的基本联系：对于频繁访问的信息丰富查询，最小描述长度与深度乘以知识库大小的对数成正比。提出了存储-计算权衡：超过临界阈值的查询缓存比重新计算更便宜。

Conclusion: 推导深度为推理复杂性提供了可计算度量，揭示了描述复杂性与推理努力之间的基本关系。提出的存储-计算权衡策略可优化缓存分配，框架可扩展到处理噪声或不完整知识库。

Abstract: We introduce derivation depth-a computable metric of the reasoning effort needed to answer a query based on a given set of premises. We model information as a two-layered structure linking abstract knowledge with physical carriers, and separate essential core facts from operational shortcuts. For any finite premise base, we define and prove the computability of derivation depth. By encoding reasoning traces and applying information-theoretic incompressibility arguments, we establish fundamental bounds linking depth to the descriptive complexity of queries. For frequently asked, information-rich queries, the minimal description length grows proportionally to depth times the logarithm of the knowledge base size. This leads to a practical storage-computation tradeoff: queries accessed beyond a critical threshold become cheaper to cache than recompute. We formulate optimal cache allocation as a mathematical optimization problem solvable with approximation guarantees and extend the framework to handle noisy or incomplete knowledge bases.

</details>


### [3] [Physics-Compliant Modeling and Optimization of MIMO Systems Aided by Microwave Linear Analog Computers](https://arxiv.org/abs/2602.19379)
*Matteo Nerini,Bruno Clerckx*

Main category: cs.IT

TL;DR: 本文为微波线性模拟计算机（MiLAC）辅助的MIMO系统建立了物理合规模型，考虑了天线互耦效应，推导了端到端系统模型，并提出了互耦感知的MiLAC优化方案，证明了互耦对MiLAC系统的有益影响。


<details>
  <summary>Details</summary>
Motivation: 现有关于MiLAC辅助通信的研究依赖于理想化信道模型并忽略天线互耦效应。然而，由于MiLAC在射频端进行处理，互耦效应变得至关重要，不仅改变信道特性，还会改变MiLAC实现的线性变换操作。

Method: 使用多端口网络理论为MiLAC辅助的MIMO系统开发物理合规模型，考虑互耦效应。推导了发射端、接收端或两端都使用MiLAC时的端到端系统模型，并制定了互耦感知的MiLAC优化问题，推导出最大化接收信号功率的闭式全局最优解。

Result: 建立了三个重要分析结果：1）互耦在MiLAC辅助系统中平均而言是有益的；2）考虑互耦时，MiLAC的性能相当于配备匹配网络的数字架构，但使用更少的射频链；3）考虑互耦时，MiLAC总是优于没有匹配网络的数字架构。数值仿真验证了理论发现。

Conclusion: 本文为MiLAC辅助的MIMO系统提供了首个考虑互耦效应的物理合规模型，证明了互耦对系统性能的积极影响，并展示了MiLAC在减少射频链数量的同时能够达到与配备匹配网络的数字架构相当的性能，为实际MiLAC系统设计提供了理论基础。

Abstract: Microwave linear analog computer (MiLAC) has emerged as a promising architecture for implementing linear multiple-input multiple-output (MIMO) processing in the analog domain, with radio frequency (RF) signals. Existing studies on MiLAC-aided communications rely on idealized channel models and neglect antenna mutual coupling. However, since MiLAC performs processing at RF, mutual coupling becomes critical and alters the implemented operation, not only the channel characteristics. In this paper, we develop a physics-compliant model for MiLAC-aided MIMO systems accounting for mutual coupling with multiport network theory. We derive end-to-end system models for scenarios with MiLACs at the transmitter, the receiver, or both, showing how mutual coupling impacts the linear transformation implemented by the MiLACs. Furthermore, we formulate and solve a mutual coupling aware MiLAC optimization problem, deriving a closed-form globally optimal solution that maximizes the received signal power. We establish the fundamental performance limits of MiLAC with mutual coupling, and derive three analytical results. First, mutual coupling is beneficial in MiLAC-aided systems, on average. Second, with mutual coupling, MiLAC performs as digital architectures equipped with a matching network, while having fewer RF chains. Third, with mutual coupling, MiLAC always outperforms digital architectures with no matching network. Numerical simulations confirm our theoretical findings.

</details>


### [4] [Toward a Quiet Wireless World: Multi-Cell Pinching-Antenna Transmission](https://arxiv.org/abs/2602.19459)
*Zhiguo Ding*

Main category: cs.IT

TL;DR: 应用pinching天线到多小区干扰管理，通过近距离传输实现低功耗"耳语"通信，替代传统高功率"喊叫"方式


<details>
  <summary>Details</summary>
Motivation: 传统天线多小区干扰管理导致功耗过高，特别是为服务小区边缘用户时，基站需要高功率传输来克服严重的大尺度路径损耗，这被称为"喊叫"式通信

Method: 采用pinching天线技术进行多小区干扰管理，通过将收发对放置在近距离位置，实现低功率传输

Result: 多小区pinching天线传输能够创建一个"安静的无线世界"，用户QoS需求可以通过低发射功率满足，实现"耳语"式通信而非高功率传输

Conclusion: pinching天线在多小区干扰管理中具有显著优势，能够大幅降低功耗，为无线通信系统提供更节能的解决方案

Abstract: Conventional-antenna-based multi-cell interference management can lead to excessive power consumption. For example, in order to serve those users which are close to the cell edge, base stations often must transmit at very high power levels to overcome severe large-scale path-loss, i.e., the base stations have to ``shout" at the users to realize the users' target quality of service (QoS). This letter focuses on the application of pinching antennas to multi-cell interference management and demonstrates that the use of multi-cell pinching-antenna transmission leads to a quiet wireless world. In particular, each transceiver pair can be positioned in close proximity, and hence the users' QoS requirements can be met with only low transmit power, i.e., via ``whispering" rather than high-power transmission.

</details>


### [5] [Physics-Aware, Shannon-Optimal Compression via Arithmetic Coding for Distributional Fidelity](https://arxiv.org/abs/2602.19476)
*Cristiano Fanelli*

Main category: cs.IT

TL;DR: 提出基于算术编码的无损压缩方法，通过比较数据集压缩长度差异来评估分布一致性，作为生成AI合成数据保真度的度量工具。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI广泛用于创建合成数据，需要严格验证这些合成数据与原始训练数据的分布一致性。传统方法面临数据量和维度增长的挑战，需要一种全局、可解释的保真度度量方法。

Method: 使用算术编码对数据集进行无损可逆压缩，基于物理信息概率表示。通过比较数据集在相同物理参考分布下的压缩长度差异来定义保真度度量，差异越大表示分布不一致性越严重。

Result: 该方法产生的度量是全局的、可解释的、分量可加的，且在香农意义下渐近最优。压缩比优于传统通用算法如gzip，并能通过压缩长度差异量化分布不一致性。

Conclusion: 基于算术编码的无损物理感知压缩不仅是数据压缩工具，更可作为测量仪器来测试数据集间的保真度，为验证生成AI合成数据的分布一致性提供有效方法。

Abstract: Assessing whether two datasets are distributionally consistent has become a central theme in modern scientific analysis, particularly as generative artificial intelligence is increasingly used to produce synthetic datasets whose fidelity must be rigorously validated against the original data on which they are trained, a task made more challenging by the continued growth in data volume and problem dimensionality. In this work, we propose the use of arithmetic coding to provide a lossless and invertible compression of datasets under a physics-informed probabilistic representation. Datasets that share the same underlying physical correlations admit comparable optimal descriptions, while discrepancies in those correlations-arising from miscalibration, mismodeling, or bias-manifest as an irreducible excess in code length. This excess codelength defines an operational fidelity metric, quantified directly in bits through differences in achievable compression length relative to a physics-inspired reference distribution. We demonstrate that this metric is global, interpretable, additive across components, and asymptotically optimal in the Shannon sense. Moreover, we show that differences in codelength correspond to differences in expected negative log-likelihood evaluated under the same physics-informed reference model. As a byproduct, we also demonstrate that our compression approach achieves a higher compression ratio than traditional general-purpose algorithms such as gzip. Our results establish lossless, physics-aware compression based on arithmetic coding not as an end in itself, but as a measurement instrument for testing the fidelity between datasets.

</details>


### [6] [Nacrith: Neural Lossless Compression via Ensemble Context Modeling and High-Precision CDF Coding](https://arxiv.org/abs/2602.19626)
*Roberto Tacconelli*

Main category: cs.IT

TL;DR: Nacrith是一个无损压缩系统，结合了135M参数的Transformer语言模型和轻量级在线预测器，通过多项技术创新在文本压缩上超越了传统压缩方法和现有神经压缩器。


<details>
  <summary>Details</summary>
Motivation: 现有LLM-based压缩器存在量化开销大、速度慢、无法处理二进制文件等问题。Nacrith旨在通过改进CDF精度、引入本地预测模型、自适应纠错等技术，实现更高效、更通用的神经压缩系统。

Method: 结合135M参数的SmolLM2-135M Transformer模型与轻量级在线预测器，采用32位算术编码器。引入8项关键技术：CDF精度提升、token级N-gram模型、自适应log-space偏置头、置信度跳转、混合二进制格式、高效推理后端、多GPU并行压缩、KV缓存滑动窗口优化。

Result: 在Canterbury Corpus的alice29.txt上达到0.918 bpb，优于gzip 3.1倍、bzip2 2.5倍、CMIX v21 44%、ts_zip 20%。在enwik8上达到0.9389 bpb，超越ts_zip 15%、FineZip 8%。在未见文本上达到0.723 bpb，证明不是记忆伪影。

Conclusion: Nacrith通过多项技术创新实现了高效的无损压缩，在压缩比和速度上都超越了现有方法，且能处理二进制文件，为神经压缩领域提供了新的解决方案。

Abstract: We present Nacrith, a lossless compression system that combines a 135M-parameter transformer language model (SmolLM2-135M) with an ensemble of lightweight online predictors and a 32-bit arithmetic coder. Beyond the base LLM-plus-arithmetic-coding paradigm, Nacrith introduces several contributions: (1) a CDF precision upgrade from 2^16 to 2^24 that eliminates ~75% of quantization overhead caused by minimum-probability floors in large vocabularies; (2) a token-level N-gram model for fast local predictions; (3) an adaptive log-space bias head correcting per-document LLM errors via online gradient descent; (4) confidence-based LLM skip for accelerating highly predictable tokens; (5) a hybrid binary format (NC06) extending neural compression to arbitrary binary files--to our knowledge a first among LLM-based compressors; (6) a llama.cpp inference backend achieving ~7x faster single-token decode than PyTorch; (7) parallel multi-GPU compression across up to 8 workers; and (8) native KV cache sliding window reducing per-slide cost by ~37x. The system requires only ~500 MB of GGUF weights and ~1.2 GB VRAM per worker, running on consumer GPUs.
  On alice29.txt (Canterbury Corpus, 152 KB), Nacrith achieves 0.918 bits per byte (bpb)--outperforming gzip by 3.1x, bzip2 by 2.5x, CMIX v21 by 44%, and ts_zip by 20%, while compressing below the 0th-, 1st-, and 2nd-order byte-level Shannon entropy bounds. On enwik8 (100 MB), Nacrith achieves 0.9389 bpb (11.74%), surpassing ts_zip (~1.11 bpb) by 15% and FineZip (1.024 bpb) by 8% despite using a 60x smaller model with no fine-tuning. An out-of-distribution evaluation on a document published after the model's training cutoff confirms these gains are not memorization artifacts, achieving 0.723 bpb on unseen text.

</details>


### [7] [Secure Communications, Sensing, and Computing Towards Next-Generation Networks](https://arxiv.org/abs/2602.19942)
*Ruiqi Liu,Beixiong Zheng,Jemin Lee,Si-Hyeon Lee,Georges Kaddoum,Onur Günlü,Deniz Gündüz*

Main category: cs.IT

TL;DR: 该论文全面调查了集成无线通信-感知-计算系统中的安全与隐私威胁及防护措施，涵盖物理层安全、语义通信、感知安全、分布式计算安全，并提出统一安全框架。


<details>
  <summary>Details</summary>
Motivation: 下一代无线网络正从单纯连接向集成感知和计算能力演进，这种融合带来了新的安全挑战。系统复杂度增加、攻击面扩大、数据密集型AI应用带来的隐私问题，都迫切需要全面研究集成无线系统的安全防护。

Method: 采用系统性调查方法：1) 回顾通信网络的物理层安全技术；2) 分析语义和语用通信的安全隐私影响及跨层设计；3) 识别感知功能在信号源、传播通道、感知目标三个层面的风险及防护策略；4) 讨论分布式计算中的数据泄露、弱认证等风险及安全编码计算方法；5) 提出面向集成通信-感知-计算架构的统一安全框架。

Result: 论文系统梳理了集成无线系统中各层面的安全威胁：通信层面的物理层攻击、语义通信漏洞；感知层面的信号源欺骗、信道干扰、目标隐私泄露；计算层面的分布式计算风险。同时总结了相应的防护技术，包括物理层安全机制、跨层设计、感知防御策略、安全编码计算等，最终提出了端到端的统一安全框架。

Conclusion: 集成通信-感知-计算系统面临复杂的安全挑战，需要跨层、跨领域的综合防护方案。论文提出的统一安全框架为未来无线系统提供了端到端保护视角，强调安全设计必须与系统架构深度融合，以应对不断演变的威胁环境。

Abstract: Next-generation wireless networks are progressing beyond conventional connectivity to incorporate emerging sensing and computing capabilities. This convergence gives rise to integrated systems that enable not only uninterrupted communication, but also environmental awareness, intelligent decision-making, and novel applications that take advantage of these combined features. At the same time, this integration brings substantial security challenges. As computing, sensing, and communication become more tightly intertwined, the overall complexity of the system increases, creating new vulnerabilities and expanding the attack surface. The widespread deployment of data-heavy artificial intelligence applications further amplifies concerns regarding data security and privacy. This paper presents a comprehensive survey of security and privacy threats, along with potential countermeasures, in integrated wireless systems. We first review physical-layer security techniques for communication networks, and then investigate the security and privacy implications of semantic and pragmatic communications and their associated cross-layer design methodologies. For sensing functionalities, we pinpoint security and privacy risks at the levels of signal sources, propagation channels, and sensing targets, and summarize state-of-the-art defense strategies for each. The growing computational requirements of these applications drive the need for distributed computing over the network, which introduces additional risks such as data leakage, weak authentication, and multiple points of failure. We subsequently discuss secure coded computing approaches that can help overcome several of these challenges. Finally, we introduce unified security frameworks tailored to integrated communication-sensing-computing architectures, offering an end-to-end perspective on protecting future wireless systems.

</details>


### [8] [Enormous Fluid Antenna Systems (E-FAS)--Part II: Channel Estimation](https://arxiv.org/abs/2602.20127)
*Farshad Rostami Ghadi,Kai-Kit Wong,Masoud Kaveh,Hao Xu,Baiyang Liu,Kin-Fai Tong,Chan-Byoung Chae*

Main category: cs.IT

TL;DR: 首次全面分析E-FAS辅助下行链路传输在基于导频的信道估计下的性能，揭示了不完美CSI下的SNR饱和现象和干扰限制特性。


<details>
  <summary>Details</summary>
Motivation: 现有研究在完美CSI下展示了E-FAS的巨大功率增益，但实际信道获取对E-FAS性能的影响尚未充分探索，需要分析不完美CSI下的系统表现。

Method: 开发了端到端信道的估计框架，推导了MMSE信道估计及其误差的统计特性闭式表达式，分析了单用户和多用户场景，考虑了训练开销。

Result: 单用户场景揭示了由残余自干扰引起的固有SNR饱和现象；多用户场景显示ZF预编码在高SNR区域因残余用户间干扰而变得干扰受限；量化了空间复用增益与导频开销的权衡。

Conclusion: 尽管存在CSI不完美和训练成本，E-FAS仍保持显著性能优势，其放大的大规模信道增益提供了鲁棒性，验证了在实际信道估计下的有效性。

Abstract: Enormous fluid antenna systems (E-FAS) have recently emerged as a new wireless architecture in which intelligent metasurfaces act as guided electromagnetic interfaces, enabling surface-wave (SW) propagation with much lower attenuation and more control than conventional space-wave transmission. While prior work has reported substantial power gains under perfect channel state information (CSI), the impact of practical channel acquisition on E-FAS performance remains largely unexplored. This paper presents the first comprehensive analysis of E-FAS-assisted downlink transmission under pilot-based channel estimation. We develop an estimation framework for the equivalent end-to-end channel and derive closed-form expressions for the statistics of the minimum mean-square-error (MMSE) channel estimate and its estimation error. Building on these results, we analyze both single-user and multiuser operation while explicitly accounting for the training overhead. For the single-user case, we characterize the outage probability and achievable rate with imperfect CSI, and reveal an inherent signal-to-noise ratio (SNR) saturation phenomenon caused by residual self-interference. For the multiuser case, we study zero-forcing (ZF) precoding based on imperfect channel estimates and show that the system becomes interference-limited in the high SNR regime because of residual inter-user interference. Furthermore, we quantify the trade-off between spatial multiplexing gains and pilot overhead when the number of users increases. Analytical findings are validated via Monte Carlo simulations and benchmarked against least-squares (LS) estimation and conventional non-E-FAS transmission. The results reveal that despite CSI imperfections and training costs, E-FAS retains substantial performance advantages and provides robustness enabled by its amplified large-scale channel gain.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [9] [ZUNA: Flexible EEG Superresolution with Position-Aware Diffusion Autoencoders](https://arxiv.org/abs/2602.18478)
*Christopher Warner,Jonas Mago,JR Huml,Mohamed Osman,Beren Millidge*

Main category: eess.SP

TL;DR: ZUNA是一个380M参数的掩码扩散自编码器，用于处理任意电极数量和位置的EEG信号，通过4D旋转位置编码实现时空结构注入，在208个公共数据集上训练，性能优于传统插值方法且具有跨数据集泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统EEG信号处理方法（如球面样条插值）在电极缺失或位置变化时性能受限，现有深度学习方法缺乏跨数据集和电极位置的泛化能力，需要一种能够处理任意电极配置的通用EEG信号处理方法。

Method: ZUNA采用掩码扩散自编码器架构，将多通道EEG信号分块为短时间窗口，使用4D旋转位置编码（x,y,z,t）注入时空结构，支持任意电极子集和位置的推理。在208个公共数据集（约200万通道小时）上训练，结合重建和重度通道丢弃目标。

Result: ZUNA显著优于常用的球面样条插值方法，在高丢弃率下优势更明显。相比其他深度学习方法，ZUNA具有跨数据集和电极位置的泛化能力，可直接应用于新数据集。虽然具备生成能力，但计算上仍保持实用。

Conclusion: ZUNA提供了一个通用的EEG信号处理框架，能够处理任意电极配置，性能优于传统方法且具有良好泛化能力。作者开源了Apache-2.0许可的权重和MNE兼容的预处理/推理栈，促进可重复比较和下游EEG分析应用。

Abstract: We present \texttt{ZUNA}, a 380M-parameter masked diffusion autoencoder trained to perform masked channel infilling and superresolution for arbitrary electrode numbers and positions in EEG signals. The \texttt{ZUNA} architecture tokenizes multichannel EEG into short temporal windows and injects spatiotemporal structure via a 4D rotary positional encoding over (x,y,z,t), enabling inference on arbitrary channel subsets and positions. We train ZUNA on an aggregated and harmonized corpus spanning 208 public datasets containing approximately 2 million channel-hours using a combined reconstruction and heavy channel-dropout objective. We show that \texttt{ZUNA} substantially improves over ubiquitous spherical-spline interpolation methods, with the gap widening at higher dropout rates. Crucially, compared to other deep learning methods in this space, \texttt{ZUNA}'s performance \emph{generalizes} across datasets and channel positions allowing it to be applied directly to novel datasets and problems. Despite its generative capabilities, \texttt{ZUNA} remains computationally practical for deployment. We release Apache-2.0 weights and an MNE-compatible preprocessing/inference stack to encourage reproducible comparisons and downstream use in EEG analysis pipelines.

</details>


### [10] [Heterogeneity-agnostic AI/ML-assisted beam selection for multi-panel arrays](https://arxiv.org/abs/2602.18678)
*Ibrahim Kilinc,Robert W. Heath*

Main category: eess.SP

TL;DR: 提出一种支持天线异构的AI/ML波束选择算法，通过预测与天线配置无关的无线传播特性，实现异构天线系统的通用波束选择。


<details>
  <summary>Details</summary>
Motivation: 异构天线硬件（不同尺寸、方向、码本、元件模式和极化角）限制了现有基于位置信息的AI/ML波束选择方法的可行性和泛化能力，需要开发能够适应天线异构性的解决方案。

Method: 1) 推导解耦传播特性与天线配置的RSRP模型；2) 提出从波束成形RSRP测量中提取传播变量（AoA、AoD、包含路径增益和信道去极化的矩阵）的优化框架；3) 开发三阶段自回归网络，从用户位置预测这些变量，从而计算任意天线配置的RSRP并进行波束选择。

Result: 仿真结果显示，该异构无关方法在有/无天线异构情况下都能提供接近理想选择的频谱效率，无需为每种配置重新训练或使用单独模型。

Conclusion: 提出的方法通过解耦传播特性与天线配置，实现了对异构天线系统的通用波束选择，解决了现有方法在异构硬件环境中的泛化限制问题。

Abstract: AI/ML-based beam selection methods coupled with location information effectively reduce beam training overhead. Unfortunately, heterogeneous antenna hardware with varying dimensions, orientations, codebooks, element patterns, and polarization angles limits their feasibility and generalization. This challenge requires either a heterogeneity-agnostic model functional under these variations, or developing many models for each configuration, which is infeasible and expensive in practice. In this paper, we propose a unifying AI/ML-based beam selection algorithm supporting antenna heterogeneity by predicting wireless propagation characteristics independent of antenna configuration. We derive a reference signal received power (RSRP) model that decouples propagation characteristics from antenna configuration. We propose an optimization framework to extract propagation variables consisting of angle-of-arrival (AoA), angle-of-departure (AoD), and a matrix incorporating path gain and channel depolarization from beamformed RSRP measurements. We develop a three-stage autoregressive network to predict these variables from user location, enabling RSRP calculation and beam selection for arbitrary antenna configurations without retraining or having a separate model for each configuration. Simulation results show our heterogeneity-agnostic method provides spectral efficiency close to that of genie-aided selection both with and without antenna heterogeneity.

</details>


### [11] [Channel-Correlation-Based Access Point Selection and Pilot Power Allocation for Cell-Free Massive MIMO](https://arxiv.org/abs/2602.18875)
*Saeed Mohammadzadeh,Rodrigo C. De Lamare,Kanapathippillai Cumanan,Hien Quoc Ngo*

Main category: eess.SP

TL;DR: 提出动态AP选择和导频功率分配框架用于上行无蜂窝大规模MIMO系统，通过分层相关聚类算法和导频功率优化来提升频谱效率


<details>
  <summary>Details</summary>
Motivation: 无蜂窝大规模MIMO系统中，用户间干扰严重限制了频谱效率，传统AP选择方法无法同时保证强信道增益和低相关性，且缺乏有效的导频功率分配策略

Method: 1) 分层相关聚类算法将AP按信道相关性分组；2) 用户与提供强信道增益且低相关性的AP关联；3) 引入用户容量约束防止硬件过载；4) 将导频功率分配建模为加权和速率最大化问题，使用二次变换迭代求解

Result: 数值结果表明，该方法显著提升频谱效率，在高密度多用户场景下保持性能，收敛速度优于基准方案，同时减少CSI估计需求和网络更新开销

Conclusion: 提出的动态AP选择和导频功率分配框架有效缓解了无蜂窝大规模MIMO系统中的用户间干扰，提高了频谱效率和系统可扩展性，为高密度网络部署提供了实用解决方案

Abstract: This paper proposes a dynamic access point (AP) selection and pilot power allocation (DAPPA) framework for uplink cell-free massive multiple-input multiple-output (CFmMIMO) systems, aiming to mitigate inter-user interference and improve overall spectral efficiency (SE). A hierarchical correlation-based clustering algorithm is developed to group APs according to their channel correlation, enabling each user to be associated with APs that simultaneously provide strong channel gains and low mutual correlation. This association ensures reliable connectivity, maximizes coherent combining gains, and reduces inter-user interference, while also allowing the number of AP clusters to be adjusted flexibly, without the need to reorganize the network completely. By maintaining links to low-correlated APs, the proposed scheme reduces the need for frequent channel state information (CSI) estimation and minimizes network-wide update overhead. To enhance scalability, a user-capacity constraint per AP is incorporated, preventing hardware overload and alleviating the effects of pilot reuse. Furthermore, an effective pilot power allocation strategy is introduced to boost the signal-to-interference-plus-noise ratio (SINR) during channel training. This is formulated as a weighted sum-rate maximization (WSRM) problem and solved iteratively using a quadratic transform, which enables efficient optimization while ensuring fairness and high-quality service across all users. Numerical results demonstrate that the proposed method delivers significant SE gains, maintains performance in high-density multi-user scenarios, and converges faster than benchmark schemes.

</details>


### [12] [A Spatial Similarity-Guided Pilot Assignment and Access Point Selection for Cell-Free Massive MIMO Networks](https://arxiv.org/abs/2602.18901)
*Saeed Mohammadzadeh,Kanapathippillai Cumanan,Pei Liu,Hien Quoc Ngo*

Main category: eess.SP

TL;DR: 提出基于信道相似性的导频分配和接入点选择策略，用于提升无蜂窝大规模MIMO系统的上行频谱效率


<details>
  <summary>Details</summary>
Motivation: 在无蜂窝大规模MIMO系统中，导频污染和接入点间干扰限制了频谱效率，特别是在密集网络部署中需要更有效的干扰管理策略

Method: 提出信道相似性感知导频分配(CAPA)策略，根据用户间信道相似性动态分配正交导频；同时设计接入点选择算法，优先选择低相关性接入点以增强空间分集

Result: 仿真结果表明，所提策略在动态用户场景下显著提升了频谱效率，特别是在密集网络部署中效果更明显

Conclusion: 通过联合优化导频分配和接入点选择，有效管理干扰并提升无蜂窝大规模MIMO系统的频谱效率，为密集网络部署提供了有效解决方案

Abstract: This paper investigates pilot assignment and access point (AP) selection strategies for uplink cell-free massive multiple-input multiple-output (CF-mMIMO) systems. We propose channel similarity-aware pilot assignment (CAPA) and AP selection schemes to improve interference management and, consequently, spectral efficiency (SE). The pilot assignment strategy dynamically allocates pilot sequences by evaluating inter-user channel similarity, ensuring that users (UEs) with high channel similarity are assigned orthogonal pilots to mitigate pilot contamination. Subsequently, an AP selection algorithm is introduced that prioritizes the selection of low-correlation APs to reduce interference and enhance spatial diversity. This selection process maintains robust UE-AP links while minimizing inter-AP redundancy. The combined approach significantly improves SE, particularly in dense network deployments. Simulation results are provided to demonstrate the effectiveness of the proposed strategies under dynamic UE scenarios.

</details>


### [13] [Event-Triggered Gossip for Distributed Learning](https://arxiv.org/abs/2602.19116)
*Zhiyuan Zhai,Xiaojun Yuan,Wei Ni,Xin Wang,Rui Zhang,Geoffrey Ye Li*

Main category: eess.SP

TL;DR: 提出基于事件触发的分布式学习框架，通过自适应通信控制机制减少节点间通信开销，相比传统全通信基线减少71.61%的点对点传输，仅带来边际性能损失。


<details>
  <summary>Details</summary>
Motivation: 分布式学习虽然为无中心协调的分布式网络提供了新的学习范式，但受到节点间通信瓶颈的限制。现有方法需要频繁的模型信息交换，导致高通信开销。

Method: 开发事件触发的gossip框架，引入自适应通信控制机制，使每个节点能够基于本地模型偏差自主决定何时与邻居交换模型信息，实现完全去中心化的通信决策。

Result: 在非凸目标函数下分析了框架的遍历收敛性，并在不同触发条件下解释收敛保证。仿真结果显示，相比最先进的分布式学习方法，显著降低通信开销，减少71.61%的累积点对点传输，仅带来边际性能损失。

Conclusion: 提出的框架有效解决了分布式学习中的通信瓶颈问题，通过事件触发机制在保持学习性能的同时大幅降低通信开销，为资源受限的分布式网络提供了实用的解决方案。

Abstract: While distributed learning offers a new learning paradigm for distributed network with no central coordination, it is constrained by communication bottleneck between nodes.
  We develop a new event-triggered gossip framework for distributed learning to reduce inter-node communication overhead. The framework introduces an adaptive communication control mechanism that enables each node to autonomously decide in a fully decentralized fashion when to exchange model information with its neighbors based on local model deviations. We analyze the ergodic convergence of the proposed framework under noconvex objectives and interpret the convergence guarantees under different triggering conditions. Simulation results show that the proposed framework achieves substantially lower communication overhead than the state-of-the-art distributed learning methods, reducing cumulative point-to-point transmissions by \textbf{71.61\%} with only a marginal performance loss, compared with the conventional full-communication baseline.

</details>


### [14] [Dual Security for MIMO-OFDM ISAC Systems: Artificial Ghosts or Artificial Noise](https://arxiv.org/abs/2602.20045)
*Yinchao Yang,Prabhat Raj Gautam,Yathreb Bouazizi,Michael Breza,Julie McCann*

Main category: eess.SP

TL;DR: 提出一个双层双安全ISAC框架，同时保护感知和通信安全，无需窃听者信道信息，通过人工噪声和人工鬼影干扰窃听者。


<details>
  <summary>Details</summary>
Motivation: ISAC系统虽然能高效共享无线资源，但也带来了新的基于感知的安全漏洞。窃听者不仅能截获通信信息，还能被动利用目标回波推断感知参数，而用户无法察觉。目前ISAC系统中感知和通信安全的联合保护尚未得到探索。

Method: 提出双层双安全ISAC框架：1）联合设计发射波束赋形器，注入人工噪声干扰通信窃听者；2）故意扭曲感知窃听者可用的参考信号以削弱其感知能力；3）生成具有虚假角度-距离-速度剖面的人工鬼影，合法接收器可抑制这些鬼影，而感知窃听者无法抑制，从而降低其正确检测真实目标的概率。

Result: 数值结果表明，所提框架能有效增强通信和感知安全性，同时保持通信用户和合法感知接收器的性能。

Conclusion: 该研究填补了ISAC系统中感知和通信安全联合保护的空白，提出的双层双安全框架无需窃听者信道状态信息，能有效应对被动感知窃听者和通信窃听者的威胁。

Abstract: Integrated sensing and communication (ISAC) enables the efficient sharing of wireless resources to support emerging applications, but it also gives rise to new sensing-based security vulnerabilities. Here, potential communication security threats whereby confidential messages intended for legitimate users are intercepted, but also unauthorized receivers (Eves) can passively exploit target echoes to infer sensing parameters without users being aware. Despite these risks, the joint protection of sensing and communication security in ISAC systems remains unexplored. To address this challenge, this paper proposes a two-layer dual-secure ISAC framework that simultaneously protects sensing and communication against passive sensing Eves and communication Eves, without requiring their channel state information (CSI). Specifically, transmit beamformers are jointly designed to inject artificial noise (AN) to introduce interference to communication Eves, while deliberately distorting the reference signal available to sensing Eves to impair their sensing capability. Furthermore, the proposed design generates artificial ghosts (AGs) with fake angle-range-velocity profiles observable by all receivers. Legitimate receivers can suppress these AGs, whereas sensing Eves cannot, thereby significantly reducing their probability of correctly detecting the true targets. Numerical results demonstrate that the proposed framework effectively enhances both communication and sensing security, while preserving the performance of communication users and legitimate sensing receivers.

</details>


### [15] [Downlink Beamforming Design for NOMA Using Convolutional Neural Networks](https://arxiv.org/abs/2602.19136)
*Chentong Li,Saeed Mohammadzadeh,Kanapathippillai Cumanan,Octavia A. Dobre*

Main category: eess.SP

TL;DR: 提出基于CNN的波束赋形设计方法，用于下行NOMA系统，以解决传统优化算法计算复杂度过高的问题，显著降低计算时间并保持接近最优性能。


<details>
  <summary>Details</summary>
Motivation: NOMA和波束赋形是实现大规模连接的关键技术，但传统最优波束赋形解决方案依赖复杂迭代算法和优化方法，计算负担和延迟高，不适合时延敏感应用。

Method: 提出基于卷积神经网络(CNN)的方法，利用信道状态信息的两种表示作为输入特征，生成归一化的波束赋形向量，解决发射功率最小化问题。

Result: 仿真结果显示，CNN解决方案接近最优标签性能，同时相比传统高复杂度算法显著减少计算时间，增强了实时应用的实用性。

Conclusion: 基于CNN的波束赋形设计方法能有效平衡性能和计算效率，为NOMA系统中的实时波束赋形提供了实用解决方案。

Abstract: Non-orthogonal multiple access (NOMA) and beamforming are well-established techniques for enabling massive connectivity in future wireless networks. However, many optimal beamforming solutions rely on highly complex iterative algorithms and optimization methods, resulting in an increase in computational burden and latency, making them less suitable for delay-sensitive applications and services. To address these challenges, we propose an effective convolutional neural network (CNN)-based approach for beamforming design in downlink NOMA systems to solve the transmit power minimization problem. The proposed method utilizes two representations of channel state information as input features to produce normalized beamforming vectors. Simulation results show that the CNN-based solution closely approximates the optimal label performance while significantly reducing computational time compared to conventional high-complexity algorithms, enhancing its practicality for real-time applications.

</details>


### [16] [A data-driven model-free physical-informed deep operator network for solving nonlinear dynamic system](https://arxiv.org/abs/2602.19262)
*Jieming Sun,Lichun Li*

Main category: eess.SP

TL;DR: 提出一种基于少量数据的无模型物理信息深度算子网络框架，用于学习非线性动态系统


<details>
  <summary>Details</summary>
Motivation: 现有物理信息深度算子网络要么需要已知系统数学公式，要么需要大量场景数据。但在某些动态系统中，难以获得精确数学公式和大量数据，只能获取少量实验数据或有限数学信息。

Method: 首先探索可用数据的短期依赖性，使用代理机器学习模型提取短期依赖性；然后将该代理模型作为物理信息部分整合到DeepOnet中；最后训练构建的DeepOnet来模拟给定控制输入和初始条件下系统的动态响应。

Result: 在不同系统上的数值实验证实，该DeepOnet框架能够有效学习近似某些非线性动态系统的动态响应。

Conclusion: 提出了一种能够从少量数据中学习非线性动态系统的数据驱动无模型物理信息深度算子网络框架，解决了传统方法需要精确数学公式或大量数据的限制。

Abstract: The existing physical-informed Deep Operator Networks are mostly based on either the well-known mathematical formula of the system or huge amounts of data for different scenarios. However, in some cases, it is difficult to get the exact mathematical formula and vast amounts of data in some dynamic systems, we can only get a few experimental data or limited mathematical information. To address the cases, we propose a data-driven model-free physical-informed Deep Operator Network (DeepOnet) framework to learn the nonlinear dynamic systems from few available data. We first explore the short-term dependence of the available data and use a surrogate machine learning model to extract the short-term dependence. Then, the surrogate machine learning model is incorporated into the DeepOnet as the physical information part. Then, the constructed DeepOnet is trained to simulate the system's dynamic response for given control inputs and initial conditions. Numerical experiments on different systems confirm that our DeepOnet framework learns to approximate the dynamic response of some nonlinear dynamic systems effectively.

</details>


### [17] [Elevation-Aware Supplementary Uplink for Direct Satellite-to-Device Communications](https://arxiv.org/abs/2602.19427)
*Rajan Shrestha,Hayder Al-Hraishawi*

Main category: eess.SP

TL;DR: 该论文提出了一种基于仰角感知的补充上行链路框架，用于增强卫星直连设备通信的上行链路鲁棒性，通过智能切换不同频段的上行载波来扩展覆盖范围并提高能效。


<details>
  <summary>Details</summary>
Motivation: 卫星直连设备通信的上行链路面临长传播距离、严重路径损耗和用户设备功率限制等挑战，特别是在低仰角和波束边缘区域可靠性较差。需要一种解决方案来增强上行链路鲁棒性同时保持用户设备的功率效率。

Method: 利用低地球轨道卫星的可预测几何特性，开发了仰角感知的补充上行链路框架。该框架基于仰角相关的链路余量估计，在不同频段间自适应调整上行链路操作。提出了一种带有迟滞的仰角感知SUL激活算法，指导上行载波选择并防止频繁切换。

Result: 仿真结果表明，所提出的SUL框架能够将有效上行链路覆盖扩展到低仰角和波束边缘区域，提高卫星通过期间的上行链路可用性，并在现实的用户设备功率约束下实现稳定操作，且上行链路切换次数最少。

Conclusion: 补充上行链路技术能够有效增强卫星直连设备通信系统的上行链路鲁棒性，仰角感知的自适应框架在扩展覆盖范围、提高可用性和保持功率效率方面表现出色，为全球卫星通信提供了可行的解决方案。

Abstract: Direct satellite-to-device (DS2D) communication enables standard mobile devices to connect directly to low Earth orbit (LEO) satellites, providing global coverage without reliance on terrestrial infrastructure. However, the DS2D uplink is fundamentally constrained by long propagation distances, severe path loss, and stringent user equipment (UE) power limits, making uplink reliability particularly challenging at low elevation angles and beam edges. This paper investigates the integration of supplementary uplink (SUL) technology into DS2D systems to enhance uplink robustness while preserving UE power efficiency. Leveraging the predictable geometry of LEO satellite orbits, we develop an elevation-aware SUL framework that adapts uplink operation across frequency bands based on elevation-dependent link margin estimates. The proposed approach schedules the UE to transmit on either a primary uplink carrier or a lower-frequency SUL carrier. An elevation-aware SUL activation algorithm with hysteresis is introduced to guide uplink carrier selection while preventing frequent switching. Simulation results demonstrate that the proposed SUL framework extends effective uplink coverage toward low-elevation and beam-edge regions, improves uplink availability over a satellite pass, and achieves stable operation with a minimal number of uplink transitions under realistic UE power constraints.

</details>


### [18] [An LLM-Enabled Frequency-Aware Flow Diffusion Model for Natural-Language-Guided Power System Scenario Generation](https://arxiv.org/abs/2602.19522)
*Zhenghao Zhou,Yiyan Li,Fei Xie,Lu Wang,Bo Wang,Jiansheng Wang,Zheng Yan,Mo-Yuen Chow*

Main category: eess.SP

TL;DR: 提出LFFD框架，使用自然语言指导电力系统场景生成，结合LLM语义理解和流扩散模型，解决传统方法用户便利性和生成灵活性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 传统AI场景生成方法（如条件生成对抗网络）依赖固定长度的数值条件向量，存在用户便利性差和生成灵活性不足的问题。需要一种能用自然语言描述生成需求的更灵活方法。

Method: 1) 引入预训练LLM模块，将自然语言生成请求转换为有序语义空间；2) 采用流扩散模型（基于修正流匹配目标）进行高效高质量场景生成；3) 训练中引入频率感知多目标优化算法缓解频率偏差问题；4) 设计双代理框架创建文本-场景训练样本对并标准化语义评估。

Result: 基于大规模光伏和负荷数据集的实验证明了该方法的有效性，能够根据自然语言描述生成所需场景。

Conclusion: 提出的LFFD框架通过自然语言指导的场景生成，提高了用户便利性和生成灵活性，为电力系统规划运行提供了更强大的场景生成工具。

Abstract: Diverse and controllable scenario generation (e.g., wind, solar, load, etc.) is critical for robust power system planning and operation. As AI-based scenario generation methods are becoming the mainstream, existing methods (e.g., Conditional Generative Adversarial Nets) mainly rely on a fixed-length numerical conditioning vector to control the generation results, facing challenges in user conveniency and generation flexibility. In this paper, a natural-language-guided scenario generation framework, named LLM-enabled Frequency-aware Flow Diffusion (LFFD), is proposed to enable users to generate desired scenarios using plain human language. First, a pretrained LLM module is introduced to convert generation requests described by unstructured natural languages into ordered semantic space. Second, instead of using standard diffusion models, a flow diffusion model employing a rectified flow matching objective is introduced to achieve efficient and high-quality scenario generation, taking the LLM output as the model input. During the model training process, a frequency-aware multi-objective optimization algorithm is introduced to mitigate the frequency-bias issue. Meanwhile, a dual-agent framework is designed to create text-scenario training sample pairs as well as to standardize semantic evaluation. Experiments based on large-scale photovoltaic and load datasets demonstrate the effectiveness of the proposed method.

</details>


### [19] [Dynamic Sensor Scheduling Based on Node Partitioning of Graphs](https://arxiv.org/abs/2602.19561)
*Ryouke Ikura,Junya Hara,Hiroshi Higashi,Yuichi Tanaka*

Main category: eess.SP

TL;DR: 提出一种基于图信号采样理论的动态传感器调度方法，通过图节点分区获得多个同等信息量的节点子集，以应对电池消耗和传感器故障，并适应动态变化。


<details>
  <summary>Details</summary>
Motivation: 在传感器网络应用中，需要多个同等信息量的节点子集依次激活，以提高网络对集中电池消耗和传感器故障的鲁棒性。同时，这些子集的质量会动态变化，需要适应这些变化。

Method: 基于图信号采样理论的图节点分区方法，将问题表述为基于图信号子空间先验的DC优化问题，使用近端DC算法求解。为适应在线场景，自适应地从历史数据估计信号子空间并顺序更新分区先验。

Result: 在合成和真实传感器网络数据上的数值实验表明，该方法相比替代方法实现了更低的平均均方误差。

Conclusion: 提出的动态传感器调度方法通过图节点分区有效解决了传感器网络中的鲁棒性和适应性需求，在理论和实验上都表现出优越性能。

Abstract: This paper proposes a dynamic sensor scheduling method for sensor networks. In sensor network applications, we often need multiple equally-informative node subsets that are activated sequentially to make a sensor network robust against concentrated battery consumption and sensor failures. In addition, quality of these subsets changes dynamically and thus we must adapt those changes. To find those node subsets, we propose a graph node partitioning method based on sampling theory for graph signals. We aim to minimize the average reconstruction error for signals obtained at all node subsets, in contrast to conventional single subset selection. The graph node partitioning problem is formulated as a difference-of-convex (DC) optimization based on a subspace prior of graph signals, and is solved by the proximal DC algorithm. It guarantees convergence to a critical point. To accommodate the online scenario where the signal subspace and optimal partitioning may change over time, we adaptively estimate the signal subspace from historical data and sequentially update the prior for our partitioning method. Numerical experiments on synthetic and real-world sensor network data demonstrate that the proposed method achieves lower average mean squared errors compared to alternative methods.

</details>


### [20] [Extracting Patterns of Chemical Information from Differential Mobility Spectrometry Measurements under Varying Conditions of Humidity and Temperature](https://arxiv.org/abs/2602.19572)
*Philipp Müller,Gary A. Eiceman,Anton Rauhameri,Anton Kontunen,Antti Roine,Niku Oksala,Antti Vehkaoja,Maiju Lepomäki*

Main category: eess.SP

TL;DR: 该研究分析了手术烟雾DMS测量对湿度和温度的依赖性，通过回归模型标准化测量数据，为消除标准化测量条件需求迈出第一步。


<details>
  <summary>Details</summary>
Motivation: DMS技术可用于分析手术烟雾，但其测量结果受湿度和温度影响，使得不同环境条件下的数据比较变得困难。传统方法通过调节环境条件来标准化测量，但这往往不可行或不可能实现。

Method: 分析了1852个猪脂肪和肌肉组织手术烟雾的DMS测量数据，确认了测量对湿度和温度的依赖性。通过拟合回归模型到原始和标准化的DMS测量数据，使用这些模型基于记录的湿度和温度来估计已知组织类型的DMS测量。

Result: 分析证实了DMS测量对湿度和温度的明显依赖性。通过按分离电压标准化DMS测量并在标准化数据上训练多元回归模型，可以在特定环境条件下估计手术烟雾的DMS测量。

Conclusion: 研究表明，通过标准化DMS测量并训练回归模型，可以估计特定环境条件下的手术烟雾DMS测量，这是消除标准化测量条件需求的第一步，为DMS技术在手术烟雾分析中的更广泛应用铺平道路。

Abstract: Differential Mobility Spectrometry (DMS), also known as Field Asymmetric Ion Mobility Spectrometry, is a rapid and affordable technology for extracting information from gas phase samples containing complex volatile organic compounds, and can therefore be used for analyzing surgical smoke. One obstacle to its widespread application is the dependence of DMS measurements on humidity and, to a lesser degree, temperature, making comparison of data measured under different environmental conditions arbitrary. The commonly used solution is to regulate these environmental conditions to some predefined humidity and temperature levels. However, this approach is often unfeasible or even impossible. Therefore, in this paper we analyzed a dataset of 1,852 DMS measurements of surgical smoke evaporated from porcine adipose and muscle tissue to get an understanding of the impact of varying humidity and temperature on DMS measurements. Our analysis confirmed clear dependence of the measurements on these two factors. To overcome this challenge, we fitted regression models to raw and normalized DMS measurement data. Subsequently, these models were used for estimating DMS measurements for known tissue types based on recorded humidity and temperatures. Our test suggests that it is possible to estimate DMS measurements of surgical smoke from porcine adipose and muscle tissue under specific environmental conditions by standardizing DMS measurements separation voltage-wise and training multivariate regression models on the normalized data, which is the first step in removing the need for standardized measurement conditions.

</details>


### [21] [Active IoT User Detection in Near-Field with Location Information](https://arxiv.org/abs/2602.19613)
*Gabriel Martins de Jesus,Richard Demo Souza,Onel Luis Alcaraz López*

Main category: eess.SP

TL;DR: 论文提出了一种利用用户位置先验知识来增强近场物联网网络中活跃用户检测性能的方法，通过重构用户的视距信道分量来辅助检测过程。


<details>
  <summary>Details</summary>
Motivation: 在近场物联网网络中，传统活跃用户检测方法性能有限。考虑到用户位置信息通常可用（静态用户可直接告知，移动用户可通过定位算法获取），利用这些位置先验知识来重构视距信道分量，有望显著提升检测性能。

Method: 基站利用用户位置估计重构其视距信道分量，结合用户导频序列增强接收信号与活跃用户之间的相关性。将位置辅助的活跃用户检测建模为凸优化问题，采用交替方向乘子法（ADMM）求解。

Result: 在完美位置估计和强视距信道条件下，所提方法显著优于基线方法。鲁棒性分析表明，只要位置估计误差保持在系统参数确定的界限内，性能增益仍然存在。但该方法计算复杂度高于不使用位置信息的基线ADMM方法。

Conclusion: 利用用户位置先验知识可以有效提升近场物联网网络中的活跃用户检测性能。该方法在位置估计准确且视距信道较强时效果显著，即使在位置估计存在一定误差的情况下仍能保持性能优势，为实际部署提供了可行性。

Abstract: In this paper, we address active users detection (AUD) in near-field Internet of Things (IoT) networks by exploring prior knowledge of users' locations. We consider a scenario where users are distributed in a semi-circular area within the Rayleigh distance of a multi-antenna base station (BS). We propose the BS to use location estimates of the users to reconstruct their line-of-sight (LoS) channel components, hence assisting the AUD process. For this, the BS combines these reconstructed channels with users' pilot sequences, enhancing the correlation between received signals and active users. We formulate the location-aided AUD as a convex optimization problem, solved via the alternating direction method of multipliers (ADMM). {Our proposal has a higher computational complexity compared to the baseline ADMM approach where location information is not used. Moreover, the proposal requires location information of users, which can be readily informed if users are static, or inferred via established localization algorithms if they are mobile.} Simulation results compare our proposal against the baseline across varying systems parameters, such as number of users, pilot length and LoS component strength. We demonstrate that under perfect location estimation and strong LoS, our proposed method significantly outperforms the baseline. Furthermore, robustness analysis shows that performance gains persist under imperfect location estimation, provided the estimation error remains within bounds determined by the system parameters.

</details>


### [22] [Topological Signal Processing for 3D Point Cloud Data](https://arxiv.org/abs/2602.19636)
*Tiziana Cattai,Stefania Sardellitti,Stefania Colonnese,Sergio Barbarossa*

Main category: eess.SP

TL;DR: 将拓扑信号处理框架应用于3D点云分析，在单纯复形上表示点云，引入高阶拉普拉斯算子处理三角网格上的信号，能够同时表征颜色属性和几何信息。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理3D点云时存在局限性，需要一种能够同时处理几何和属性信息的统一框架。拓扑信号处理为点云分析提供了新的数学工具，但需要将其扩展到3D点云场景。

Method: 基于离散外微积分理论，在单纯复形上表示3D点云，引入高阶拉普拉斯算子处理三角网格信号。将颜色属性建模为节点上的3D向量，几何信息建模为三角形重心上的3D向量，将点云属性作为边信号进行处理。

Result: 在合成点云上的数值实验表明，该方法能够准确重建颜色信息，对稀疏数据具有鲁棒性，在噪声点云坐标情况下能够进行几何细化。拓扑信号处理工具能够有效用于点云的采样、恢复和滤波。

Conclusion: 提出的方法为点云的几何和属性表征提供了基于拓扑的表示框架，将拓扑信号处理成功应用于3D点云分析，为点云处理提供了新的数学工具。

Abstract: Our goal in this paper is to apply the topological signal processing (TSP) framework to the analysis of 3D Point Clouds (PCs) represented on simplicial complexes. Building on Discrete Exterior Calculus (DEC) theory for vector fields, we introduce higher-order Laplacian operators that enable the processing of signals over triangular meshes. Unlike traditional approaches, the proposed approach allows us to characterize both color attributes, modeled as 3D vectors on nodes, and geometry, modeled as 3D vectors on the barycenter of each triangle. Then, we show as TSP tools may efficiently be used to sample, recover and filter PCs attributes treating them as edge signals. Numerical results on synthetic PCs demonstrate accurate color reconstruction with robustness to sparse data and geometry refinement in the case of noisy PC coordinates. The proposed approach provides a topology-based representation to characterize the geometry and attributes of PCs.

</details>


### [23] [Hardware-Accelerated Geometrical Simulation of Biological and Engineered In-Air Ultrasonic Systems](https://arxiv.org/abs/2602.19652)
*Wouter Jansen,Jan Steckel*

Main category: eess.SP

TL;DR: SonoTraceUE是一个基于Unreal Engine的高保真声学仿真框架，通过硬件加速的射线追踪和蒙特卡洛衍射模型，实现动态多材料环境中近实时的主动和被动声学传感仿真。


<details>
  <summary>Details</summary>
Motivation: 当前工业监测和自主机器人中空中声学传感器的部署日益增长，但现有仿真框架难以高效模拟高频声波在大型、动态、复杂环境中的传播。基于波的方法计算成本高，而几何声学求解器又缺乏对动态场景、复杂衍射或闭环机器人集成的支持。

Method: 开发了SonoTraceUE作为Unreal Engine插件，采用硬件加速的射线追踪镜面反射模型和基于曲率的蒙特卡洛衍射模型，支持动态多材料环境中的近实时声学传感仿真。

Result: 通过生物声学研究和机器人实验两个领域验证，结果显示SonoTraceUE能够实现与真实世界频谱和空间数据的高度相关性。

Conclusion: 该框架为合成数据生成、生物声学假设测试以及使用声学传感的闭环机器人系统的快速原型设计提供了一个多功能平台。

Abstract: The deployment of in-air acoustic sensors for industrial monitoring and autonomous robotics has grown significantly, often drawing inspiration from biological echolocation. However, developing and validating these systems in existing simulation frameworks remains challenging due to the computational cost of simulating high-frequency wave propagation in large, dynamic, and complex environments. While wave-based methods offer high accuracy, they scale poorly with frequency and volume. Conversely, existing geometric acoustic solvers often lack support for dynamic scenes, complex diffraction, or closed-loop robotic integration. In this work, we introduce SonoTraceUE, a high-fidelity acoustic simulation framework built as a plugin for Unreal Engine. By using a hardware-accelerated ray tracing-based specular reflection model, and a curvature-based Monte Carlo diffraction model, the system enables near real-time simulation of active and passive acoustic sensing in dynamic, multi-material environments. We validate the framework through two distinct experimental domains: a bioacoustic study and a robotics experiment. Our results demonstrate that SonoTraceUE achieves high correlation with real-world spectral and spatial data. The framework provides a versatile platform for synthetic data generation, hypothesis testing in bioacoustics, and the rapid prototyping of closed-loop robotic systems that use acoustic sensing.

</details>


### [24] [Breaking the CP Limit: Robust Long-Range OFDM Sensing via Interference Cleaning](https://arxiv.org/abs/2602.19877)
*Umut Utku Erdem,Lucas Giroto,Benedikt Geiger,Taewon Jeong,Silvio Mandelli,Christian Karle,Benjamin Nuss,Laurent Schmalen,Thomas Zwick*

Main category: eess.SP

TL;DR: 该论文提出了两种解决OFDM雷达中超出循环前缀范围目标检测问题的方法，通过联合干扰消除和全重建滑动窗口方案，在计算成本和检测性能之间提供灵活权衡。


<details>
  <summary>Details</summary>
Motivation: 在OFDM雷达和集成感知通信系统中，传统感知范围受限于循环前缀对应的往返时间。超出此范围的目标回波会引起符号间干扰(ISI)和载波间干扰(ICI)，显著降低检测性能，提高雷达图像中的干扰噪声基底，并因窗口失配而减少有用信号功率。现有方法在恢复有用信号和抑制干扰之间存在权衡，特别是在多目标场景中。

Method: 提出了两种框架：1) 联合干扰消除与相干补偿方法，这是连续干扰消除算法的高效演进，利用高精度线性调频Z变换估计和频域相干补偿来恢复弱远距离目标；2) 全重建滑动窗口方案，通过移动接收窗口捕获最优信号能量，同时对所有检测到的目标执行全信号重建。

Result: 数值结果表明，两种方法都优于最先进的基准方法。联合干扰消除方法能有效对抗ISI和ICI引起的干扰噪声基底增加，而全重建滑动窗口方案在需要最大精度的场景中表现优异。

Conclusion: 该论文提出的两种框架解决了OFDM雷达中超出循环前缀范围目标检测的困境，提供了计算成本和目标检测性能之间的灵活权衡。联合干扰消除方法适用于一般场景，而全重建滑动窗口方案适用于需要最高精度的应用。

Abstract: In orthogonal frequency-division multiplexing-based radar and integrated sensing and communication systems, the sensing range is traditionally limited by the round-trip time corresponding to the cyclic prefix duration. Targets whose echoes arrive after this duration induce intersymbol interference (ISI) and associated intercarrier interference (ICI), which significantly degrade detection performance, elevate the interference-noise floor in the radar image, and reduce the useful signal power due to window mismatch. Existing methods face a trade-off between recovering useful signal and suppressing interference, particularly in multi-target scenarios. This paper proposes two frameworks to resolve this dilemma, offering a flexible trade-off between computational cost and target detection performance. First, a signal model is derived, demonstrating that ISI and ICI-oriented interference often dominates thermal noise in high-dynamic-range scenarios. To combat the ISI and ICI-based interference-noise floor increase, joint-interference cancellation with coherent compensation is proposed. This approach is an efficient evolution of the successive-interference cancellation algorithm, utilizing high-precision chirp Z-transform estimation and frequency-domain coherent compensation to recover weak distant targets. For scenarios requiring maximum precision, the full reconstruction-based sliding window scheme is presented, which shifts the receive window to capture optimal signal energy while performing full-signal reconstruction for all detected targets. Numerical results show that both methods outperform state-of-the-art benchmarks.

</details>


### [25] [Rethinking Chronological Causal Discovery with Signal Processing](https://arxiv.org/abs/2602.19903)
*Kurt Butler,Damian Machlanski,Panagiotis Dimitrakopoulos,Sotirios A. Tsaftaris*

Main category: eess.SP

TL;DR: 本文研究因果发现方法对采样率与时间窗口长度不匹配的敏感性，发现传统和现代方法都对此超参数敏感，建议从信号处理角度理解此现象。


<details>
  <summary>Details</summary>
Motivation: 现实世界因果发现通常基于规则时间间隔的观测数据，但这些观测时间可能与底层生物或物理事件的实际发生时间不匹配。研究者想探究这种时间不匹配对因果发现方法性能的影响。

Method: 通过实证和理论分析，研究采样率和时间窗口长度变化对因果发现性能的影响。考察了经典和近期因果发现方法对这些超参数的敏感性。

Result: 研究表明，无论是经典还是近期的因果发现方法都对采样率和时间窗口长度这些超参数表现出敏感性。性能变化与这些参数设置密切相关。

Conclusion: 因果发现方法对观测时间与底层事件时间不匹配问题敏感，需要从信号处理的角度来理解和解决这一问题，以提高方法的鲁棒性。

Abstract: Causal discovery problems use a set of observations to deduce causality between variables in the real world, typically to answer questions about biological or physical systems. These observations are often recorded at regular time intervals, determined by a user or a machine, depending on the experiment design. There is generally no guarantee that the timing of these recordings matches the timing of the underlying biological or physical events. In this paper, we examine the sensitivity of causal discovery methods to this potential mismatch. We consider empirical and theoretical evidence to understand how causal discovery performance is impacted by changes of sampling rate and window length. We demonstrate that both classical and recent causal discovery methods exhibit sensitivity to these hyperparameters, and we discuss how ideas from signal processing may help us understand these phenomena.

</details>


### [26] [From High-Level Requirements to KPIs: Conformal Signal Temporal Logic Learning for Wireless Communications](https://arxiv.org/abs/2602.20018)
*Jiechen Chen,Michele Polese,Osvaldo Simeone*

Main category: eess.SP

TL;DR: C-STLL框架结合信号时序逻辑与保形校准，从RAN KPI数据中学习可解释的时序公式，以关联低层指标与高层QoE要求，并提供形式化可靠性保证。


<details>
  <summary>Details</summary>
Motivation: 软体化无线接入网络（如O-RAN）产生丰富的KPI数据，但如何将这些低层指标与高层质量体验（QoE）要求联系起来是一个挑战。现有方法需要同时具备相关性（能捕捉预测用户级结果的时序模式）和可解释性（提供操作人员可验证和执行的见解）。

Method: 提出保形信号时序逻辑学习（C-STLL）框架：1）利用信号时序逻辑（STL）学习区分满足与不满足高层QoE要求的KPI轨迹的可解释公式；2）基于Learn Then Test框架的保形校准程序包裹现有STL学习算法，通过多重假设检验验证的接受和停止规则，联合优化可靠性、公式复杂性和多样性。

Result: 在ns-3网络模拟器的移动游戏场景实验中，C-STLL能有效将风险控制在目标水平以下，同时返回紧凑、多样的可解释时序规范集，成功关联KPI行为与QoE结果。

Conclusion: C-STLL为从RAN KPI数据中提取可操作智能提供了可靠且可解释的框架，通过形式化保证的STL公式学习，弥合了低层测量与高层网络优化要求之间的差距。

Abstract: Softwarized radio access networks (RANs), such as those based on the Open RAN (O-RAN) architecture, generate rich streams of key performance indicators (KPIs) that can be leveraged to extract actionable intelligence for network optimization. However, bridging the gap between low-level KPI measurements and high-level requirements, such as quality of experience (QoE), requires methods that are both relevant, capturing temporal patterns predictive of user-level outcomes, and interpretable, providing human-readable insights that operators can validate and act upon. This paper introduces conformal signal temporal logic learning (C-STLL), a framework that addresses both requirements. C-STLL leverages signal temporal logic (STL), a formal language for specifying temporal properties of time series, to learn interpretable formulas that distinguish KPI traces satisfying high-level requirements from those that do not. To ensure reliability, C-STLL wraps around existing STL learning algorithms with a conformal calibration procedure based on the Learn Then Test (LTT) framework. This procedure produces a set of STL formulas with formal guarantees: with high probability, the set contains at least one formula achieving a user-specified accuracy level. The calibration jointly optimizes for reliability, formula complexity, and diversity through principled acceptance and stopping rules validated via multiple hypothesis testing. Experiments using the ns-3 network simulator on a mobile gaming scenario demonstrate that C-STLL effectively controls risk below target levels while returning compact, diverse sets of interpretable temporal specifications that relate KPI behavior to QoE outcomes.

</details>


### [27] [Digital Twin--Driven Adaptive Wavelet Strategy for Efficient 6G Backbone Network Telemetry](https://arxiv.org/abs/2602.20034)
*Alexandre Barbosa de Lima,Xavier Hesselbach,José Roberto de Almeida Amazonas*

Main category: eess.SP

TL;DR: 该论文建立了MERA张量网络与paraunitary滤波器组的等价关系，提出了一种学习自适应小波的方法，在保证完美重构和能量守恒的同时，在LRD网络流量压缩上优于经典小波。


<details>
  <summary>Details</summary>
Motivation: 经典正交小波虽然保证完美重构，但基于固定基函数，对具有分形频谱特征的信号压缩效果不佳；而现有学习方法通常通过软惩罚强制正交性，牺牲了结构保证。

Method: 建立MERA张量网络与paraunitary滤波器组的严格等价关系，通过流形约束优化学习自适应小波，强制精确正交性，保证完美重构和能量守恒。

Result: 在LRD网络流量验证中，学习到的滤波器比经典小波在PSNR上提升0.5-3.8dB（六条MAWI骨干链路，2020-2025，314Mbps-1.75Gbps），同时保持Hurst指数在估计误差范围内（|ΔH|≤0.03）。

Conclusion: MERA启发的小波为6G数字孪生同步中的遥测压缩提供了一种原理性方法，在保持信号统计特性的同时实现更好的压缩性能。

Abstract: Classical orthogonal wavelets guarantee perfect reconstruction but rely on fixed bases optimized for polynomial smoothness, achieving suboptimal compression on signals with fractal spectral signatures. Conversely, learned methods offer adaptivity but typically enforce orthogonality via soft penalties, sacrificing structural guarantees.
  This work establishes a rigorous equivalence between Multiscale Entanglement Renormalization Ansatz (MERA) tensor networks and paraunitary filter banks. The resulting framework learns adaptive wavelets while enforcing exact orthogonality through manifold-constrained optimization, guaranteeing perfect reconstruction and energy conservation throughout training.
  Validation on Long-Range Dependent (LRD) network traffic demonstrates that learned filters outperform classical wavelets by 0.5--3.8~dB PSNR on six MAWI backbone traces (2020--2025, 314~Mbps--1.75~Gbps) while preserving the Hurst exponent within estimation uncertainty ($|ΔH| \le 0.03$). These results establish MERA-inspired wavelets as a principled approach for telemetry compression in 6G digital twin synchronization.

</details>


### [28] [On the Spatial Consistency of Sub-Terahertz Channel Characteristics for Beyond-6G Systems](https://arxiv.org/abs/2602.20039)
*Hossein Amininasab,Huda Farooqui,Dmitri Moltchanov,Sergey Andreev,Michele Polese,Mikko Valkama,Josep M. Jornet*

Main category: eess.SP

TL;DR: 该研究通过实验测量验证了在140-150 GHz频段室内环境中，信道特性（时延扩展、角度时延扩展、K因子）在数十厘米距离内变化很小，这意味着在稳定视距方向可以采用较粗的网格（10-50波长）进行射线追踪建模，从而显著降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 在sub-THz频段（100-300 GHz）进行射线追踪信道建模时，由于波长接近毫米级，理论上需要极细的网格分辨率，导致计算量巨大。然而实际中信道特性可能在更大距离上保持相似，本研究旨在通过实验验证sub-THz信道特性的空间一致性程度，为降低计算复杂度提供依据。

Method: 在140-150 GHz频段的室内大厅环境中进行了大规模测量活动，测量了从2.5毫米到1米不同分离距离下的信道特性，分析了时延扩展、角度时延扩展和K因子等参数的空间变化规律。

Result: 实验结果表明，在考虑的室内环境中，信道特性在数十厘米距离内变化很小。在稳定视距方向，网格分辨率可以放宽到10-50个波长（145 GHz），而在非视距主导区域则需要更细的分辨率。对于较粗的网格，需要高级插值方法来捕捉快速变化的散射分量。

Conclusion: sub-THz信道特性在空间上具有较高的一致性，这为降低射线追踪建模的计算复杂度提供了可能。在实际应用中，可以根据信道特性（视距/非视距）采用不同的网格分辨率策略，在保持建模精度的同时显著减少计算量。

Abstract: Ray tracing is a versatile approach for precise sub-terahertz (sub-THz, 100-300 GHz) channel modeling when designing new mechanisms for beyond-6G cellular systems. Theoretically, wireless channels may exhibit variations over wavelength distances. In the sub-THz band, close-to-millimeter wavelengths thus require extremely large computational efforts for ray-tracing modeling. However, in practice, channel characteristics may remain quantitatively similar over much larger distances, which can drastically decrease computational efforts. The aim of this study is to experimentally characterize the degree of spatial consistency in sub-THz channel characteristics. To this end, we performed a large-scale measurement campaign in the 140-150 GHz frequency band in an indoor-hall (InH) environment and characterized the channel at separation distances from 2.5 mm up to 1 m. Our results show that channel characteristics including delay spread, angular delay spread, and K-factor change only slightly over multiple tens of centimeter distances. This implies that, in the considered InH environment, the mesh grid can be in the range of 10-50 wavelengths (at 145 GHz) along stable line-of-sight (LoS) directions, while a finer resolution is needed in regions not dominated by LoS. For coarser grids, advanced interpolation is required to capture rapidly varying scattered components.

</details>
